{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook de Exploração\n",
    "\n",
    "Nesse arquivo, realizamos uma análise detalhada e experimentação com diversos modelos de aprendizado de máquina para determinar qual oferece o melhor desempenho para a nossa tarefa específica. Este processo inclui a preparação e a limpeza dos dados, o balanceamento das classes, a extração de características relevantes e a avaliação de múltiplos algoritmos. Diversos testes e validações são conduzidos para refinar os modelos e otimizar seus parâmetros. O objetivo final é identificar o modelo mais eficaz e robusto para ser utilizado em produção, garantindo uma análise precisa e confiável dos dados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baixar Pandas para importar CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Notebooks/dados.csv')\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Exploratória do Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introdução**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A análise exploratória de dados é um passo fundamental para desenvolver modelos que classificam sentimentos, especialmente quando lidamos com dados complexos como comentários de usuários. Neste projeto, o grupo Moodfy estudou um conjunto de comentários sobre a Uber retirados da plataforma X (antes conhecida como Twitter). O objetivo dessa análise inicial era entender melhor os dados, descobrir padrões e preparar a base para criar um modelo que possa classificar os comentários como positivos, negativos ou neutros. Esse trabalho inicial é crucial para garantir que o modelo final seja preciso e eficaz, influenciando decisões importantes sobre como atender e se comunicar com os clientes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicação dos Passos da Análise Exploratória Feita pelo Grupo Moodfy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Distribuição de Sentimentos: O grupo Moodfy começou analisando como os sentimentos estavam distribuídos entre os comentários para ver se havia um equilíbrio entre positivos, negativos e neutros. Essa checagem é importante porque um desequilíbrio pode fazer o modelo futuro pender para o lado mais comum.\n",
    "\n",
    "2. Análise de Palavras Comuns em Comentários: Eles identificaram quais palavras apareciam mais em cada tipo de sentimento. Esse passo ajuda a entender quais temas são frequentes e como certas palavras podem influenciar a classificação dos sentimentos.\n",
    "\n",
    "3. Análise de Comprimento dos Comentários: O grupo investigou se o tamanho dos comentários estava relacionado com os sentimentos expressados. Comentários mais longos podem indicar sentimentos mais fortes.\n",
    "\n",
    "4. Frequência de Palavras por Sentimento: Essa etapa detalhou mais a análise anterior, quantificando quantas vezes certas palavras apareciam nos diferentes sentimentos. Isso foi útil para ver se algumas palavras muito comuns, que não adicionam muito significado, estavam influenciando os resultados.\n",
    "\n",
    "5. Correlação entre Comprimento do Comentário e Sentimento: Eles também verificaram se havia uma relação entre o tamanho dos comentários e o tipo de sentimento, para entender se pessoas mais satisfeitas ou insatisfeitas tendem a escrever mais.\n",
    "\n",
    "6. Média de Comprimento dos Comentários por Sentimento: Por último, o grupo calculou o comprimento médio dos comentários para cada sentimento, buscando identificar comentários muito longos ou curtos que fogem do comum, o que pode sugerir a necessidade de ajustar os dados antes de criar o modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importação de bibliotecas de análise gráfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise Exploratória do Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criação de Dataframe exclusivo para análise exploratória do corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.read_csv('..Notebooks/dados.csv')\n",
    "\n",
    "df_analysis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Distribuição de Sentimentos**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este gráfico de barras mostra a frequência de cada categoria de sentimento nos dados. Os sentimentos estão categorizados como -1 (negativo), 0 (neutro) e 1 (positivo). O gráfico ajuda a visualizar rapidamente a proporção de comentários em cada categoria, permitindo uma análise quantitativa rápida da natureza geral dos comentários no dataset. A visualização utiliza cores diferentes para cada categoria para facilitar a distinção: vermelho para negativo, cinza para neutro e azul para positivo. Esta análise é baseada no autoestudo \"Como Fazer Análise de Sentimentos com Dados Textuais\", onde aprendemos a aplicar métodos estatísticos para entender a distribuição de sentimentos em dados textuais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentiment_distribution(df_analysis):\n",
    "    \"\"\"\n",
    "    Cria um gráfico de barras para mostrar a distribuição dos sentimentos nos comentários.\n",
    "    \n",
    "    Args:\n",
    "        df_analysis (DataFrame): DataFrame contendo os dados dos sentimentos.\n",
    "    \n",
    "    Returns:\n",
    "        None: Exibe o gráfico de barras com a distribuição dos sentimentos.\n",
    "    \"\"\"\n",
    "    # Calcula a contagem de cada sentimento\n",
    "    sentiment_counts = df_analysis['sentiment'].value_counts().sort_index()\n",
    "\n",
    "    # Gráfico de barras\n",
    "    plt.bar(sentiment_counts.index, sentiment_counts.values, color=['red', 'grey', 'blue'])\n",
    "    plt.xlabel('Sentimento')\n",
    "    plt.ylabel('Frequência de comentários')\n",
    "    plt.title('Distribuição de Sentimentos')\n",
    "    plt.xticks([-1, 0, 1], ['Negativo', 'Neutro', 'Positivo'])\n",
    "    plt.show()\n",
    "\n",
    "plot_sentiment_distribution(df_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Análise de Palavras Comuns em Comentários**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando a técnica de nuvem de palavras, este gráfico destaca as palavras mais comuns em comentários de diferentes sentimentos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(data, title):\n",
    "    \"\"\"\n",
    "    Gera uma nuvem de palavras a partir de um conjunto de dados de texto.\n",
    "\n",
    "    Args:\n",
    "    data (Series): Dados de texto (pandas Series) a partir dos quais gerar a nuvem de palavras.\n",
    "    title (str): Título da nuvem de palavras.\n",
    "\n",
    "    Returns:\n",
    "    None: A função exibe a nuvem de palavras usando matplotlib.\n",
    "    \"\"\"\n",
    "    # Garantir que todos os itens em `data` sejam strings\n",
    "    data = [' '.join(item) if isinstance(item, list) else item for item in data]\n",
    "\n",
    "    wc = WordCloud(background_color='white', max_words=200)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wc.generate(' '.join(data)), interpolation='bilinear')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Gerar Word Clouds para cada sentimento\n",
    "generate_wordcloud(df_analysis[df_analysis['sentiment'] == -1]['comment'], 'Palavras mais comuns em comentários negativos')\n",
    "generate_wordcloud(df_analysis[df_analysis['sentiment'] == 0]['comment'], 'Palavras mais comuns em comentários neutros')\n",
    "generate_wordcloud(df_analysis[df_analysis['sentiment'] == 1]['comment'], 'Palavras mais comuns em comentários positivos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Análise de Comprimento dos Comentários**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este gráfico de caixa (boxplot) ilustra a distribuição do comprimento dos comentários com base nos sentimentos expressos, categorizados em negativo (-1), neutro (0) e positivo (1). Cada caixa no gráfico representa a distribuição dos comprimentos de comentários para uma categoria de sentimento específica, oferecendo uma visão sobre a mediana, os quartis e os valores extremos (outliers) de comprimento para cada grupo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comment_length_by_sentiment(df_analysis):\n",
    "    \"\"\"Gera um gráfico de caixa para visualizar a distribuição do comprimento dos comentários por sentimento.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame contendo as colunas 'comment' e 'sentiment', onde 'comment' são os comentários\n",
    "                        e 'sentiment' são os sentimentos associados aos comentários.\n",
    "\n",
    "    Returns:\n",
    "        None: Exibe o gráfico de caixa mostrando o comprimento dos comentários distribuídos por sentimentos.\n",
    "    \"\"\"\n",
    "    # Calcula o comprimento de cada comentário\n",
    "    df_analysis['comment_length'] = df_analysis['comment'].apply(len)\n",
    "\n",
    "    # Configura e exibe o gráfico de caixa\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='sentiment', y='comment_length', data=df_analysis)\n",
    "    plt.title('Comprimento dos Comentários por Sentimento')\n",
    "    plt.xlabel('Sentimento')\n",
    "    plt.ylabel('Comprimento do Comentário (N° de caraceteres)')\n",
    "    plt.xticks(ticks=[0, 1, 2], labels=['Negativo (-1)', 'Neutro (0)', 'Positivo (1)'])\n",
    "    plt.show()\n",
    "\n",
    "# chamada da função:\n",
    "plot_comment_length_by_sentiment(df_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Frequência de Palavras por Sentimento**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este gráfico de barras compara a frequência das palavras mais comuns em comentários negativos e neutros. As barras indicam quantas vezes cada palavra foi mencionada, proporcionando uma visualização clara das diferenças no vocabulário usado em diferentes estados emocionais. Analisar essas frequências ajuda a entender os temas predominantes e possíveis áreas de insatisfação ou discussões gerais que não envolvem sentimentos fortes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_word_frequency_pipeline(df):\n",
    "    \"\"\"Analisa e visualiza as palavras mais comuns em comentários categorizados por sentimentos negativos, neutros e positivos.\n",
    "\n",
    "    Args:\n",
    "        df_analysis (DataFrame): DataFrame contendo as colunas 'comment' e 'sentiment', onde 'comment' são os comentários\n",
    "                        e 'sentiment' identifica o sentimento do comentário.\n",
    "\n",
    "    Returns:\n",
    "        None: Gera gráficos de barras mostrando as palavras mais comuns para cada categoria de sentimento.\n",
    "    \"\"\"\n",
    "    # Função para contar palavras em um texto\n",
    "    def count_words(text):\n",
    "        if isinstance(text, str):  # Se for uma string, divide em palavras\n",
    "            return Counter(text.split())\n",
    "        elif isinstance(text, list):  # Se for uma lista, já está tokenizado\n",
    "            return Counter(text)\n",
    "        else:\n",
    "            raise ValueError(\"O tipo de entrada não é suportado.\")\n",
    "\n",
    "    # Aplicação da função de contagem de palavras e agregação por sentimento\n",
    "    df['words'] = df['comment'].apply(count_words)\n",
    "    negative_words = sum(df[df['sentiment'] == -1]['words'], Counter())\n",
    "    neutral_words = sum(df[df['sentiment'] == 0]['words'], Counter())\n",
    "    positive_words = sum(df[df['sentiment'] == 1]['words'], Counter())\n",
    "\n",
    "    # Seleção das 10 palavras mais comuns em cada categoria de sentimento\n",
    "    most_common_neg = negative_words.most_common(10)\n",
    "    most_common_neu = neutral_words.most_common(10)\n",
    "    most_common_pos = positive_words.most_common(10)\n",
    "\n",
    "    # Criação de gráficos de barras para cada categoria de sentimento\n",
    "    fig, ax = plt.subplots(3, 1, figsize=(10, 8))\n",
    "\n",
    "    ax[0].bar([word[0] for word in most_common_neg], [word[1] for word in most_common_neg], color='red')\n",
    "    ax[0].set_title('Palavras mais comuns em comentários negativos')\n",
    "    ax[0].set_ylabel('Frequência De Repetição da Palavra')\n",
    "\n",
    "    ax[1].bar([word[0] for word in most_common_neu], [word[1] for word in most_common_neu], color='grey')\n",
    "    ax[1].set_title('Palavras mais comuns em comentários neutros')\n",
    "    ax[1].set_ylabel('Frequência De Repetição da Palavra')\n",
    "\n",
    "    ax[2].bar([word[0] for word in most_common_pos], [word[1] for word in most_common_pos], color='green')\n",
    "    ax[2].set_title('Palavras mais comuns em comentários positivos')\n",
    "    ax[2].set_ylabel('Frequência De Repetição da Palavra')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# chamada da função com df_analysis\n",
    "analyze_word_frequency_pipeline(df_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Correlação entre Comprimento do Comentário e Sentimento**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este gráfico de dispersão explora se o comprimento dos comentários varia com o sentimento expresso. Cada ponto representa um comentário, posicionado de acordo com seu sentimento (negativo, neutro) e comprimento. A visualização ajuda a identificar se comentários mais longos tendem a ser negativos, positivos, ou neutros, fornecendo insights sobre como os usuários se expressam em diferentes contextos emocionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comment_length_sentiment_correlation(df_analysis):\n",
    "    \"\"\"Gera um gráfico de dispersão para examinar a correlação entre o comprimento dos comentários e os sentimentos expressos.\n",
    "\n",
    "    Args:\n",
    "        df_analysis (DataFrame): DataFrame contendo as colunas 'sentiment' e 'comment_length', onde 'sentiment' indica o sentimento\n",
    "                        do comentário e 'comment_length' é o comprimento do comentário medido em número de caracteres.\n",
    "\n",
    "    Returns:\n",
    "        None: Exibe o gráfico de dispersão.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(df_analysis['sentiment'], df_analysis['comment_length'], alpha=0.5)\n",
    "    plt.title('Correlação entre Comprimento do Comentário e Sentimento')\n",
    "    plt.xlabel('Tipo de Sentimento')\n",
    "    plt.ylabel('Comprimento do Comentário - (N° Caracteres)')\n",
    "    plt.xticks([-1, 0, 1], ['Negativo', 'Neutro', 'Positivo'])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# chamada da função:\n",
    "plot_comment_length_sentiment_correlation(df_analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Média de Comprimento dos Comentários por Sentimento**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse é um gráfico de barras que mostra a média de comprimento dos comentários para cada categoria de sentimento. Este gráfico pode indicar se sentimentos mais fortes (positivos ou negativos) levam a comentários mais detalhados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_average_comment_length_by_sentiment(df_analysis):\n",
    "    \"\"\"Calcula e visualiza a média de comprimento dos comentários por sentimento em um gráfico de barras.\n",
    "\n",
    "    Args:\n",
    "        df_analysis (DataFrame): DataFrame contendo as colunas 'sentiment' e 'comment_length', onde 'sentiment' indica o sentimento\n",
    "                        do comentário e 'comment_length' é o comprimento do comentário.\n",
    "\n",
    "    Returns:\n",
    "        None: Exibe o gráfico de barras mostrando a média de comprimento dos comentários para cada sentimento.\n",
    "    \"\"\"\n",
    "    # Cálculo da média de comprimento dos comentários por sentimento\n",
    "    average_lengths = df_analysis.groupby('sentiment')['comment_length'].mean()\n",
    "\n",
    "    # Configuração e exibição do gráfico de barras\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=average_lengths.index, y=average_lengths.values, palette='coolwarm')\n",
    "    plt.title('Média de Comprimento dos Comentários por Sentimento')\n",
    "    plt.xlabel('Sentimento')\n",
    "    plt.ylabel('Média de Comprimento do Comentário (N° de Caracteres)')\n",
    "    plt.xticks(ticks=[0, 1, 2], labels=['Negativo (-1)', 'Neutro (0)', 'Positivo (1)'])\n",
    "    plt.show()\n",
    "\n",
    "# chamada da função:\n",
    "plot_average_comment_length_by_sentiment(df_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-Processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O pré-processamento é uma etapa fundamental em um modelo de machine learning, responsável por preparar e organizar os dados antes de serem alimentados ao algoritmo de aprendizado. Ele inclui uma série de técnicas e procedimentos, como normalização, padronização, tratamento de dados faltantes e seleção de características relevantes. A funcionalidade do pré-processamento é melhorar a qualidade dos dados, tornando-os mais adequados e representativos para o modelo de machine learning. Isso ajuda a evitar problemas como overfitting, garantir que o modelo possa generalizar bem para dados novos e melhorar a eficácia do aprendizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from spellchecker import SpellChecker\n",
    "import contractions\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost as xgb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Baixar os recursos necessários do NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    \"\"\"\n",
    "    Expande contrações em um texto dado com base em um mapeamento fornecido.\n",
    "\n",
    "    Args:\n",
    "        text (str): Texto que contém contrações.\n",
    "        contraction_mapping (dict): Um dicionário mapeando contrações para suas formas expandidas.\n",
    "\n",
    "    Returns:\n",
    "        str: Texto com todas as contrações expandidas de acordo com o mapeamento.\n",
    "    \"\"\"\n",
    "    contraction_map = {\n",
    "    \"can't\": \"can not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"they'll\": \"they will\",\n",
    "}\n",
    "    \n",
    "    for contraction, expansion in contraction_map.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "    return text\n",
    "\n",
    "# Exemplo de uso\n",
    "sample_text = \"I can't do this anymore, because it's too hard.\"\n",
    "expanded_text = expand_contractions(sample_text)\n",
    "print(expanded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenização é o processo de dividir um texto em unidades menores chamadas tokens. Esses tokens podem ser palavras individuais, partes de palavras ou até mesmo caracteres, dependendo do nível de granularidade desejado. A tokenização é uma etapa fundamental no processamento de linguagem natural (PLN), sendo essencial para a preparação, análise e manipulação de texto em uma variedade de aplicações, incluindo análise de sentimento, classificação de texto e tradução automática. Ao dividir o texto em tokens, os dados tornam-se estruturados e adequados para análise, facilitando a extração de informações e a modelagem de soluções de PLN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokeniza o DataFrame em uma lista de tokens.\n",
    "\n",
    "    Args:\n",
    "    text (DataFrame): Comentários a serem tokenizados.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de tokens.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "def filter_empty_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Remove listas vazias ou com espaços vazios.\n",
    "\n",
    "    Args:\n",
    "    tokens (list): Lista de tokens.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de tokens não vazios.\n",
    "    \"\"\"\n",
    "    return [token for token in tokens if token.strip()]\n",
    "\n",
    "result = tokenize_text(\"Amanda asked about apples and bananas at the market.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correção de Texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função spell_checker é projetada para corrigir a ortografia de um texto fornecido, garantindo que certas palavras específicas, como \"uber\", não sejam alteradas indevidamente. Utilizando a biblioteca TextBlob, a função separa o texto em palavras individuais e aplica a correção ortográfica em cada uma delas. No entanto, se a palavra for \"uber\" (em qualquer capitalização), ela é mantida inalterada. Após a correção de todas as palavras, o texto é reconstruído e retornado com a ortografia corrigida. Este método é útil para manter a integridade de palavras específicas enquanto melhora a precisão ortográfica geral do texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_checker(words):\n",
    "    \"\"\"\n",
    "    Corrige a ortografia de uma lista de palavras, preservando a palavra 'uber'.\n",
    "\n",
    "    Args:\n",
    "    words (list of str): Lista de palavras a serem corrigidas.\n",
    "\n",
    "    Returns:\n",
    "    str: Texto com a ortografia corrigida.\n",
    "    \"\"\"\n",
    "    corrected_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if word.lower() == 'uber':\n",
    "            corrected_words.append(word)\n",
    "        else:\n",
    "            corrected_word = str(TextBlob(word).correct())\n",
    "            corrected_words.append(corrected_word)\n",
    "    \n",
    "    corrected_text = ' '.join(corrected_words)\n",
    "    return corrected_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lemmatização é o processo de reduzir palavras a sua forma base ou lema, considerando o contexto e a morfologia da língua. Essa técnica é importante em PLN para tratar diferentes formas de uma palavra como iguais, como \"corre\" e \"correu\" ambas reduzidas a \"correr\". Isso simplifica o processamento de texto e melhora a análise em tarefas como recuperação de informações e análise de sentimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o modelo de idioma inglês do spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Retorna o tag correspondente do WordNet para o tag do Treebank do Penn.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN \n",
    "\n",
    "def lemmatize_tokens_with_pos(tokens):\n",
    "    \"\"\"\n",
    "    Lemmatiza uma lista de tokens baseando-se em sua parte do discurso.\n",
    "\n",
    "    Args:\n",
    "    tokens (list of str): Tokens a serem lematizados.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de lemas das palavras.\n",
    "    \"\"\"\n",
    "    # Criar um Doc do spaCy a partir dos tokens\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "\n",
    "    # Lemmatiza usando o spaCy\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    return lemmas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retirar pontuações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A remoção de pontuação é um processo utilizado no pré-processamento de texto que visa eliminar caracteres de pontuação, como vírgulas, pontos e pontos de exclamação, de um texto. Isso é feito para limpar o texto e reduzir a dimensionalidade dos dados, facilitando a análise e a modelagem. Ao remover a pontuação, os tokens resultantes contêm apenas palavras ou partes de palavras, tornando-os mais adequados para tarefas de processamento de linguagem natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_from_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Remove pontuações de uma lista de tokens e exclui os tokens que consistem exclusivamente de caracteres de pontuação.\n",
    "\n",
    "    Args:\n",
    "    tokens (list): Lista de tokens a serem processados.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de tokens sem pontuações.\n",
    "    \"\"\"\n",
    "    # Regex para identificar pontuações\n",
    "    regex_punctuation = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "    # Remove pontuações de cada token e filtra tokens que ficaram vazios ou são apenas pontuações\n",
    "    tokens_no_punct = [regex_punctuation.sub('', token) for token in tokens]\n",
    "    tokens_no_punct = [token for token in tokens_no_punct if token.strip() != '']\n",
    "\n",
    "    return tokens_no_punct\n",
    "\n",
    "# Exemplo de uso da função:\n",
    "tokens = [\"hello!\", \"world...\", \"#amazing\", \"test,\", \":)\", \"a\", \".\"]\n",
    "filtered_tokens = remove_punctuation_from_tokens(tokens)\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retirar números"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A remoção de números é um passo comum no pré-processamento de texto que consiste em eliminar todos os caracteres numéricos de um texto. Isso é feito para limpar o texto de informações numéricas que podem não ser relevantes para a análise ou para garantir que as palavras sejam tratadas de maneira uniforme durante a tokenização. Ao remover números, os tokens resultantes contêm apenas palavras e outros caracteres não numéricos, simplificando o texto para análise e modelagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers_from_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Remove todos os dígitos de uma lista de tokens e remove os tokens que consistem exclusivamente de números.\n",
    "\n",
    "    Args:\n",
    "    tokens (list): Lista de tokens a serem processados.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de tokens sem números.\n",
    "    \"\"\"\n",
    "    # Remover dígitos de cada token e depois filtrar os tokens que são apenas números ou ficaram vazios\n",
    "    tokens_no_numbers = [re.sub(r'\\d+', '', token) for token in tokens]\n",
    "    tokens_no_numbers = [token for token in tokens_no_numbers if token.strip() != '']\n",
    "    return tokens_no_numbers\n",
    "\n",
    "# Exemplo de uso da função:\n",
    "tokens = [\"hello123\", \"world\", \"2023\", \"test\", \"12345\"]\n",
    "filtered_tokens = remove_numbers_from_tokens(tokens)\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção de Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords são palavras que são frequentemente removidas durante o pré-processamento de texto em tarefas de Processamento de Linguagem Natural (PLN). Essas palavras são geralmente as mais comuns em um idioma, como ‘é’, ‘em’, ‘um’, ‘e’ em português, ou ‘is’, ‘in’, ‘a’, ‘and’ em inglês, e tendem a aparecer em quase todos os documentos de um corpus.\n",
    "\n",
    "A remoção de stopwords é uma prática comum porque essas palavras, embora muito frequentes, geralmente não carregam muito significado e podem adicionar ruído aos dados. Além disso, removendo-as, podemos reduzir o tamanho do nosso vocabulário e, consequentemente, o espaço de recursos, tornando nossos modelos de PLN mais eficientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixando as stopwords e o POS tagger\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def remove_stopwords_preserve_adverbs(tokens):\n",
    "    \"\"\"\n",
    "    Remove stopwords de uma lista de tokens, preservando todos os advérbios, advérbios de negação específicos,\n",
    "    e palavras essenciais em contextos negativos.\n",
    "\n",
    "    Args:\n",
    "    tokens (list): Lista de tokens a serem processados.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de tokens sem stopwords, com todos os advérbios, advérbios de negação específicos,\n",
    "          e palavras essenciais em contextos negativos preservados.\n",
    "    \"\"\"\n",
    "    if not isinstance(tokens, list):\n",
    "        raise TypeError(\"Input tokens must be a list\")\n",
    "    \n",
    "    # Carregando as stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Advérbios de negação específicos e palavras essenciais em contextos negativos para sempre preservar\n",
    "    essential_negation_words = {'not', 'never', 'no', 'nothing', 'nowhere', 'neither', 'nor'}\n",
    "    \n",
    "    # Classificando os tokens por partes do discurso\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    \n",
    "    # Filtrando os tokens para remover as stopwords, mas manter advérbios e palavras essenciais em contextos negativos\n",
    "    filtered_tokens = [word for word, tag in tagged_tokens if word.lower() not in stop_words or tag.startswith('RB') or word.lower() in essential_negation_words]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "# Exemplo de uso da função:\n",
    "tokens = [\"I\", \"do\", \"not\", \"really\", \"like\", \"never\", \"very\", \"much\", \"this\", \"car\", \"nor\", \"do\", \"I\", \"hate\", \"it\"]\n",
    "cleaned_tokens = remove_stopwords_preserve_adverbs(tokens)\n",
    "print(cleaned_tokens) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção de links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A remoção de links é uma etapa de pré-processamento em tarefas de Processamento de Linguagem Natural (PLN) que envolve a eliminação de URLs ou links de um texto. Isso é feito para reduzir o ruído nos dados e focar nas palavras e frases que carregam mais significado.\n",
    "\n",
    "Links geralmente não contribuem para a semântica de um texto e podem ser bastante variados e únicos, o que pode adicionar ruído e aumentar a dimensionalidade dos dados. Além disso, os links podem levar a conteúdo externo que está fora do contexto do texto atual, tornando a análise mais complexa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a função para remover URLs\n",
    "def remove_urls(text):\n",
    "    \"\"\"\n",
    "    Remove URLs de um texto fornecido.\n",
    "\n",
    "    Args:\n",
    "    texto (list): O texto de entrada contendo URLs.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de tokens com URLs removidas.\n",
    "    \"\"\"\n",
    "    return re.sub(r'http\\S+|www.\\S+', '', text, flags=re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanceamento das Classes de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O balanceamento das classes de dados é uma prática crucial em aprendizado de máquina para garantir que cada categoria seja representada de forma equitativa. Isso previne que o modelo desenvolva viés em direção à classe majoritária e melhora a precisão geral em prever categorias menos representadas. \n",
    "\n",
    "Na base de dados em questão, realizamos um balanceamento focado nas dinâmicas de desproporção entre as classes de sentimentos negativos e positivos. Primeiro, removemos os outliers baseando-nos apenas na distribuição dos próprios dados negativos para eliminar comentários atípicos que poderiam distorcer a análise. Após essa filtragem, procedemos com uma redução estratégica, eliminando aleatoriamente 40% dos dados negativos restantes. Além disso, para tratar a sub-representação dos sentimentos positivos, duplicamos os dados dessa classe. Essa abordagem combinada não só equilibra a presença das classes no dataset mas também facilita o treinamento de modelos mais justos e eficazes, evitando o viés em direção a qualquer classe específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_balance_negatives_and_multiply_positives(df):\n",
    "    \"\"\"\n",
    "    Remove outliers do comprimento dos comentários para a classe de sentimentos negativos (-1),\n",
    "    baseando-se nos quartis da própria classe negativa, e remove aleatoriamente 40% dos dados negativos\n",
    "    restantes para ajudar no balanceamento das classes. Multiplica os dados da classe positiva (1) por 2,5 vezes\n",
    "    para aumentar sua representatividade no dataset.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame contendo as colunas 'sentiment', 'comment', e 'comment_length'.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame com outliers removidos da classe negativa, redução estratégica de 40% dos negativos,\n",
    "                    e multiplicação dos dados positivos por 2,5 vezes.\n",
    "    \"\"\"\n",
    "    # Adiciona a coluna 'comment_length' ao DataFrame se não existir\n",
    "    if 'comment_length' not in df.columns:\n",
    "        df['comment_length'] = df['comment'].apply(len)\n",
    "\n",
    "    # Adiciona a coluna 'word_count'\n",
    "    df['word_count'] = df['comment'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    # Filtra os dados para a classe negativa (-1)\n",
    "    negativos = df[df['sentiment'] == -1]\n",
    "    \n",
    "    # Calcula os quartis apenas para a classe negativa\n",
    "    Q1 = negativos['comment_length'].quantile(0.25)\n",
    "    Q3 = negativos['comment_length'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Filtra os outliers na classe negativa baseado no limite calculado\n",
    "    negativos_filtrados = negativos[negativos['comment_length'] <= upper_bound]\n",
    "    \n",
    "    # Amostragem aleatória para remover 40% dos dados negativos filtrados\n",
    "    negativos_reduzidos = negativos_filtrados.sample(frac=0.75, random_state=42)\n",
    "\n",
    "    # Multiplica os dados da classe positiva (1) por 2,5 vezes\n",
    "    positivos = df[df['sentiment'] == 1]\n",
    "    positivos_multiplicados = pd.concat([positivos] * 2 + [positivos.sample(frac=0.5, random_state=42)], ignore_index=True)\n",
    "    \n",
    "    # Combina os dados reduzidos de sentimentos negativos com as outras classes\n",
    "    df_final = pd.concat([negativos_reduzidos, df[df['sentiment'] == 0], positivos_multiplicados], ignore_index=True)\n",
    "\n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline é um conceito essencial em diversos campos, incluindo tecnologia da informação, manufatura, logística e até mesmo em processos criativos. Em termos gerais, refere-se a uma sequência de etapas interconectadas e ordenadas, onde o resultado de cada etapa serve como entrada para a próxima. Essas etapas podem ser atividades, operações ou processos específicos. O objetivo principal de um pipeline é otimizar a eficiência e a produtividade, dividindo um trabalho complexo em partes menores e mais gerenciáveis, permitindo que cada etapa seja executada de forma independente e simultânea. Isso não apenas acelera o processo, mas também permite melhorias contínuas em cada etapa individualmente, resultando em um produto final de maior qualidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o pipeline para pré-processamento dos comentários\n",
    "pipeline = Pipeline([\n",
    "    ('expand_contractions', FunctionTransformer(lambda x: x.apply(expand_contractions))),\n",
    "    ('url_remover', FunctionTransformer(lambda x: x.apply(remove_urls))),\n",
    "    ('tokenizer', FunctionTransformer(lambda x: x.apply(tokenize_text))),\n",
    "    ('lemmatizacao', FunctionTransformer(lambda x: x.apply(lemmatize_tokens_with_pos))),\n",
    "    ('punctuation_remover', FunctionTransformer(lambda x: x.apply(remove_punctuation_from_tokens))),\n",
    "    ('number_remover', FunctionTransformer(lambda x: x.apply(remove_numbers_from_tokens))),\n",
    "    ('stopwords_remover', FunctionTransformer(lambda x: x.apply(remove_stopwords_preserve_adverbs))),\n",
    "    ('filter_empty_tokens', FunctionTransformer(lambda x: x.apply(filter_empty_tokens))),\n",
    "    ('spell_checker', FunctionTransformer(lambda x: x.apply(spell_checker))),\n",
    "])\n",
    "\n",
    "# Primeiro aplicar a remoção de outliers e a redução de dados negativos\n",
    "df = remove_outliers_balance_negatives_and_multiply_positives(df)\n",
    "\n",
    "# Aplicar o pipeline ao DataFrame existente\n",
    "df['comment'] = pipeline.fit_transform(df['comment'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exportar CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = 'pipeline_comments.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Arquivo CSV exportado para {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise dos Corpus Após o Pré-Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sentiment_distribution(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_wordcloud(df[df['sentiment'] == -1]['comment'], 'Palavras mais comuns em comentários negativos')\n",
    "generate_wordcloud(df[df['sentiment'] == 0]['comment'], 'Palavras mais comuns em comentários neutros')\n",
    "generate_wordcloud(df[df['sentiment'] == 1]['comment'], 'Palavras mais comuns em comentários positivos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comment_length_by_sentiment(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chamada da função com df\n",
    "analyze_word_frequency_pipeline(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comment_length_sentiment_correlation(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_average_comment_length_by_sentiment(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função manual\n",
    "\n",
    "A função `manual_bow` é projetada para construir manualmente uma matriz Bag of Words a partir de uma lista de documentos, onde cada documento é representado como uma lista de palavras (tokens). O processo de implementação envolve várias etapas que contribuem para a sua funcionalidade e precisão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_bow(docs):\n",
    "    \"\"\"\n",
    "    Constrói uma matriz Bag of Words (BoW) manualmente a partir de uma lista de documentos.\n",
    "\n",
    "    Args:\n",
    "        docs (list of list of str): Uma lista de documentos, onde cada documento é uma lista de palavras (tokens).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Uma tupla contendo a matriz BoW e o vocabulário ordenado.\n",
    "    \"\"\"\n",
    "    # Construção do vocabulário com os documentos\n",
    "    vocabulary = set(word for doc in docs for word in doc)\n",
    "    vocabulary = sorted(vocabulary)\n",
    "    \n",
    "    # Criação do índice para cada palavra no vocabulário\n",
    "    word_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "\n",
    "    # Construção da matriz \n",
    "    bow_matrix = []\n",
    "    for doc in docs:\n",
    "        doc_vec = [0] * len(vocabulary)\n",
    "        for word in doc:\n",
    "            if word in word_index:\n",
    "                doc_vec[word_index[word]] += 1\n",
    "        bow_matrix.append(doc_vec)\n",
    "    return bow_matrix, vocabulary\n",
    "\n",
    "# Verifica se o primeiro elemento na coluna 'comment' é uma string\n",
    "# Se for uma string, aplica uma função para dividir a string em tokens\n",
    "if isinstance(df['comment'].iloc[0], str):\n",
    "    df['comment'] = df['comment'].apply(lambda x: x.split())\n",
    "\n",
    "bow, vocab = manual_bow(df['comment'])\n",
    "\n",
    "print(\"Bag of Words Matrix (excerpt) - manual:\\n\", bow[:20])\n",
    "print(\"Vocabulary (excerpt) - manual:\\n\", vocab[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando a biblioteca _sklearn_\n",
    "\n",
    "A função `sklearn_bow` é uma implementação da construção de uma matriz Bag of Words (BoW) utilizando o `CountVectorizer` padrão da biblioteca `scikit-learn`. Esta função é projetada para processar uma lista de documentos, onde cada documento é inicialmente uma lista de palavras (tokens), e os converte em uma matriz BoW padronizada e eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_bow(docs):\n",
    "    \"\"\"\n",
    "    Constrói uma matriz Bag of Words (BoW) usando o sklearn a partir de uma lista de documentos tokenizados.\n",
    "    Cada documento deve ser uma lista de palavras (tokens), e esta função os converte de volta para strings.\n",
    "\n",
    "    Args:\n",
    "        docs (list of list of str): Uma lista de documentos, onde cada documento é uma lista de palavras (tokens).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Uma tupla contendo a matriz BoW e o vocabulário ordenado.\n",
    "    \"\"\"\n",
    "    # Converter cada documento de lista de tokens para uma string\n",
    "    docs = [\" \".join(doc) for doc in docs]\n",
    "\n",
    "    # Inicializar o CountVectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "\n",
    "    # Ajustar o modelo ao documento e transformar os documentos em uma matriz BoW\n",
    "    bow_matrix = vectorizer.fit_transform(docs).toarray()\n",
    "\n",
    "    # Obter o vocabulário ordenado\n",
    "    vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "    return bow_matrix, vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando Bigrams SkLearn\n",
    "\n",
    "A função `sklearn_bow_bigrams` é uma implementação para construir uma matriz Bag of Words (BoW) usando o `CountVectorizer` do scikit-learn, especificamente configurado para incluir bigramas. Esta abordagem permite capturar mais contexto ao considerar combinações de duas palavras consecutivas, além dos unigramas comuns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_bow_bigrams(docs):\n",
    "    \"\"\"\n",
    "    Constrói uma matriz Bag of Words (BoW) usando o sklearn a partir de uma lista de documentos tokenizados,\n",
    "    utilizando n-gramas. Cada documento deve ser uma lista de palavras (tokens), e esta função os converte de volta para strings.\n",
    "\n",
    "    Args:\n",
    "        docs (list of str): Uma lista de documentos, onde cada documento é uma string de palavras (tokens).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Uma tupla contendo a matriz BoW e o vocabulário ordenado.\n",
    "    \"\"\"\n",
    "    # Remover documentos vazios\n",
    "    docs = [doc for doc in docs if doc.strip() != '']\n",
    "\n",
    "    # Inicializar o CountVectorizer para usar 1 a 4-gramas\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 4))\n",
    "\n",
    "    # Ajustar o modelo ao documento e transformar os documentos em uma matriz BoW\n",
    "    bow_matrix = vectorizer.fit_transform(docs).toarray()\n",
    "\n",
    "    # Obter o vocabulário ordenado\n",
    "    vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "    return bow_matrix, vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando Tfidf\n",
    "\n",
    "A função `sklearn_tfidf` é uma implementação para construir uma matriz TF-IDF (Term Frequency-Inverse Document Frequency) usando o `TfidfVectorizer` do scikit-learn. Este método é utilizado em análises de texto para avaliar a importância de uma palavra em um conjunto de documentos, considerando não apenas a frequência da palavra no documento, mas também sua raridade em toda a coleção de documentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_tfidf(docs):\n",
    "    \"\"\"\n",
    "    Constrói uma matriz TF-IDF (Term Frequency-Inverse Document Frequency) usando o sklearn a partir de uma lista de documentos tokenizados.\n",
    "    Cada documento deve ser uma lista de palavras (tokens), e esta função os converte de volta para strings.\n",
    "\n",
    "    Args:\n",
    "        docs (list of list of str): Uma lista de documentos, onde cada documento é uma lista de palavras (tokens).\n",
    "\n",
    "    Returns:\n",
    "        tuple: Uma tupla contendo a matriz TF-IDF e o vocabulário ordenado.\n",
    "    \"\"\"\n",
    "    # Converter cada documento de lista de tokens para uma string\n",
    "    docs = [\" \".join(doc) for doc in docs]\n",
    "\n",
    "    # Inicializar o TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Ajustar o modelo ao documento e transformar os documentos em uma matriz TF-IDF\n",
    "    tfidf_matrix = vectorizer.fit_transform(docs).toarray()\n",
    "\n",
    "    # Obter o vocabulário ordenado\n",
    "    vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "    return tfidf_matrix, vocabulary\n",
    "\n",
    "# Exemplo de uso da função\n",
    "docs = [[\"this\", \"is\", \"the\", \"first\", \"document\"], [\"this\", \"document\", \"is\", \"the\", \"second\", \"document\"], [\"and\", \"this\", \"is\", \"the\", \"third\", \"one\"]]\n",
    "tfidf, vocab = sklearn_tfidf(docs)\n",
    "\n",
    "print(\"TF-IDF Matrix:\\n\", tfidf)\n",
    "print(\"Vocabulary:\\n\", vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilizando a biblioteca _keras_\n",
    "\n",
    "A função `generate_bow_from_tokenized_docs` é uma implementação para a criação de uma matriz Bag of Words (BoW) utilizando o `Tokenizer` do Keras, uma ferramenta do TensorFlow voltada para o processamento de texto em projetos de deep learning. Esta função processa uma lista de documentos onde cada documento já está tokenizado, e os converte em uma matriz BoW junto com o vocabulário associado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bow_from_tokenized_docs(docs):\n",
    "    \"\"\"\n",
    "    Gera a matriz Bag of Words e o vocabulário a partir de documentos tokenizados usando o Tokenizer do Keras.\n",
    "\n",
    "    Args:\n",
    "        docs (list of list of str): Lista de documentos, onde cada documento é uma lista de tokens.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Tupla contendo a matriz Bag of Words e o vocabulário.\n",
    "    \"\"\"\n",
    "    # Assegura que cada lista de tokens seja convertida de volta para uma string\n",
    "    docs_text = [' '.join(tokens) for tokens in docs]\n",
    "\n",
    "    # Inicialização do Tokenizer\n",
    "    tokenizer = Tokenizer()\n",
    "\n",
    "    # Fit no texto processado para construir o vocabulário\n",
    "    tokenizer.fit_on_texts(docs_text)\n",
    "\n",
    "    # Geração do Bag of Words\n",
    "    bow_keras = tokenizer.texts_to_matrix(docs_text, mode='count')\n",
    "\n",
    "    # Obtenção do vocabulário com índices\n",
    "    vocab_keras = {k: v for k, v in sorted(tokenizer.word_index.items(), key=lambda item: item[1])}\n",
    "\n",
    "    return bow_keras, vocab_keras\n",
    "\n",
    "# Exemplo de uso da função:\n",
    "docs = [[\"this\", \"is\", \"the\", \"first\", \"document\"], [\"this\", \"document\", \"is\", \"the\", \"second\", \"document\"], [\"and\", \"this\", \"is\", \"the\", \"third\", \"one\"]]\n",
    "bow_keras, vocab_keras = generate_bow_from_tokenized_docs(docs)\n",
    "\n",
    "print(\"Bag of Words Matrix (excerpt) - keras:\\n\", bow_keras[:20])  \n",
    "print(\"Vocabulary (excerpt) - keras:\\n\", list(vocab_keras.items()))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparação entre os técnicas de processamento de texto (BoW)\n",
    "\n",
    "Para a comparação dos modelos BoW (Bag of Words) realizados, considerasse alguns critérios de avaliação comuns. Dentre esses critérios foram selecionados: a completude do vocabulário, a densidade da matriz, a facilidade de implementação e uso e a integração com outras ferramentas de processamento de dados e aprendizado de máquina.\n",
    "\n",
    "1. Completude do vocabulário:\n",
    "    * Manual:  O vocabulário pode não incluir todas as palavras que um \"CountVectorizer\" ou \"Tokenizer\" do keras poderia extrair devido a diferenças na forma como tokens são contados ou até mesmo filtrados.\n",
    "    * Sklearn: O \"CountVectorizer\" é mais robusto e trata de muitos aspectos do pré-processamento de texto, garantindo um vocabulário mais completo.\n",
    "    * Keras: Similar ao sklearn, o \"Tokenizer\" da biblioteca keras é eficaz para extrair um vocabulário abrangente e acaba sendo ainda mais útil se o próximo passo envolver modelagem de deep learning.\n",
    "    * Sklearn (Bigramas): Captura mais contexto ao incluir combinações de palavras adjacentes, aumentando a completude e relevância do vocabulário.\n",
    "    * Sklearn (TF-IDF): Além de capturar o vocabulário, pondera as palavras pela sua raridade nos documentos, o que pode revelar termos distintivamente significativos.\n",
    "2. Densidade da matriz:\n",
    "    * Manual: Tende a produzir uma matriz com mais zeros, especialmente se o pré-processamento não é tão completo quanto os métodos automáticos.\n",
    "    * Sklearn e Keras: Geralmente eles acabam produzindo matrizes mais semelhantes em termos de densidade, mas isso pode variar dependendo do pré-processamento aplicado antes da vetorização.\n",
    "3. Facilidade de implementação e uso:\n",
    "    * Manual: Requer mais código e cuidado para garantir que os aspectos sejam realizados corretamente.\n",
    "    * Sklearn: Maior facilidade de usar e integrar com pipelines de machine learning existentes em Python, pela existência do ecossistema scikit-learn.\n",
    "    * Keras: Também apresenta maior facilidade de uso, especialmente se integrado com modelos de deep learning, mas pode ser um pouco excessivo para tarefas simples de vetorização de texto, acaba sendo mais lento para se processar.\n",
    "4. Integração com ferramentas de Machine Learning:\n",
    "    * Manual: Pode ser menos compatível com algumas ferramentas avançadas de machine learning se não houver um trabalho adicional.\n",
    "    * Sklearn: Ótima integração com outras ferramentas de machine learning em Python.\n",
    "    * Keras: Ótima escolha se o projeto for evoluir para utilizar redes neurais e deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escolha do modelo\n",
    "\n",
    "Para projetos de processamento de linguagem natural (NLP), como análise de sentimentos e classificação de texto, o uso do sklearn acaba sendo a melhor escolha devido à sua simplicidade e grande integração com o ecossistema Python para ciência de dados. A biblioteca sklearn possui uma variedade de ferramentas pré-construídas que simplificam a implementação de NLP, reduzindo a necessidade de codificação extensiva e facilitando a execução do projeto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert Vetorização - Embadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Notebooks/pipeline_comments.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O código define uma função bert_embeddings que usa o modelo BERT para converter uma lista de documentos de texto em uma matriz de embeddings. Cada documento é primeiro tokenizado e então transformado em um vetor de características pelo BERT, capturando o embedding da primeira token ([CLS]) para representar o documento inteiro. Os embeddings são acumulados e retornados como uma matriz numpy, onde cada linha corresponde ao embedding de um documento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_embeddings(docs):\n",
    "    \"\"\"\n",
    "    Constrói uma matriz de embeddings usando o BERT a partir de uma lista de documentos.\n",
    "    Cada documento deve ser uma string e esta função usa o BERT para gerar os embeddings.\n",
    "\n",
    "    Args:\n",
    "        docs (list of str): Uma lista de documentos, onde cada documento é uma string.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Uma matriz onde cada linha é o embedding de um documento.\n",
    "    \"\"\"\n",
    "    # Carregar o tokenizer e o modelo BERT\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "    model = BertModel.from_pretrained('bert-large-cased')\n",
    "\n",
    "    # Tokenizar e criar embeddings para cada documento\n",
    "    embeddings = []\n",
    "    for doc in docs:\n",
    "        inputs = tokenizer(doc, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        outputs = model(**inputs)\n",
    "        # Pegar o embedding da [CLS] token (primeira posição)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
    "        embeddings.append(cls_embedding)\n",
    "    \n",
    "    # Converter a lista de embeddings em uma matriz numpy\n",
    "    embeddings_matrix = np.vstack(embeddings)\n",
    "    \n",
    "    return embeddings_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foi usado um código para transformar listas de palavras em strings únicas na coluna 'comment' de um DataFrame df. Após essa transformação, foram impressos o número total de documentos na coluna 'comment' e o número total de sentimentos na coluna 'sentiment'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['comment'] = df['comment'].apply(lambda x: ' '.join(x))\n",
    "print(f\"Número de documentos: {len(df['comment'])}\")\n",
    "print(f\"Número de sentimentos: {len(df['sentiment'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foi gerada uma matriz de embeddings dos comentários na coluna 'comment' do DataFrame df e foram impressas suas dimensões junto com o número total de sentimentos na coluna 'sentiment'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_matrix = bert_embeddings(df['comment'].tolist())\n",
    "# Verificar as  dimensões\n",
    "print(f\"Dimensões da matriz de embeddings: {embeddings_matrix.shape}\")\n",
    "print(f\"Dimensões dos sentimentos: {len(df['sentiment'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foi gerado um array de sentimentos a partir da coluna 'sentiment' do DataFrame df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = df['sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foi usado um código para mapear os rótulos de sentimentos para classes numéricas começando de 0, utilizando o LabelEncoder do scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapear os rótulos para classes começando de 0\n",
    "label_encoder = LabelEncoder()\n",
    "mapped_sentiments = label_encoder.fit_transform(sentiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos ML - Bow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gerar o BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta etapa, utilizamos funções criadas na seção de cima para criar um (BoW). Isso significa que montamos uma lista com todas as palavras diferentes que aparecem nas avaliações dos usuários sobre o Uber. Essa lista é importante porque nos ajuda a entender quais palavras são comuns e como elas estão distribuídas nos comentários. Com essa informação, podemos preparar os dados para treinar nosso modelo de análise de sentimentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar a matriz BoW e o vocabulário\n",
    "bow, vocab = sklearn_bow_bigrams(df['comment'])\n",
    "\n",
    "# Extrair a coluna de sentimentos\n",
    "sentiments = df['sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dividir os Dados em Conjuntos de Treino e Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa fase, separamos os dados coletados em dois grupos: um para treinar nosso modelo e outro para testá-lo depois. Isso ajuda a garantir que nosso modelo aprenda com um conjunto de dados e seja capaz de fazer boas previsões sobre dados novos e desconhecidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(bow, sentiments, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Treinar os Modelos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta etapa, usamos dois tipos diferentes de modelos para entender melhor os sentimentos expressos nas avaliações do Uber:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo é bom para lidar com palavras e textos. Ele assume que cada palavra contribui de forma independente para o sentimento da avaliação, o que nos ajuda a calcular a probabilidade de um comentário ser positivo ou negativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar e treinar o modelo Naive Bayes\n",
    "bayes_model = GaussianNB()\n",
    "bayes_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Svm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo tenta encontrar a melhor linha (ou fronteira) que separa os comentários positivos dos negativos. É como desenhar uma linha reta no meio de pontos em um gráfico para distinguir entre duas categorias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(kernel='linear') \n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Avaliar o Modelo Naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após treinar nossos modelos, o próximo passo é testar como eles performam com dados que eles nunca viram antes. Para isso, seguimos estes passos:\n",
    "\n",
    "1. **Prever os Sentimentos no Conjunto de Teste**: Usamos o modelo Naive Bayes, que foi treinado anteriormente, para prever os sentimentos nas avaliações do conjunto de teste. Essas previsões são armazenadas em `y_pred`.\n",
    "\n",
    "2. **Calcular a Precisão**: A precisão é uma medida que nos diz qual porcentagem das previsões estava correta. Calculamos isso usando a função `accuracy_score`, comparando as previsões `y_pred` com as verdadeiras respostas `y_test`.\n",
    "\n",
    "3. **Gerar e Imprimir o Relatório de Classificação**: O relatório de classificação fornece mais detalhes sobre a performance do modelo, como precisão, recall e a pontuação F1 para cada classe (positivo, negativo). Usamos a função `classification_report` para obter e imprimir este relatório.\n",
    "\n",
    "Essas métricas nos ajudam a entender quão bem o modelo está trabalhando e em quais áreas ele pode ser melhorado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prever os sentimentos no conjunto de teste\n",
    "y_pred = bayes_model.predict(X_test)\n",
    "\n",
    "# Calcular a precisão\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Gerar e imprimir o relatório de classificação\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Avaliar Modelo SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de treinar o modelo SVM, precisamos verificar como ele se comporta com dados que não foram usados no treinamento. Veja os passos a seguir:\n",
    "\n",
    "1. **Prever os Sentimentos no Conjunto de Teste**: Utilizamos o modelo SVM para fazer previsões sobre as avaliações no conjunto de teste. Guardamos essas previsões na variável `y_pred_svm`.\n",
    "\n",
    "2. **Calcular a Precisão**: Para entender quão precisas são essas previsões, calculamos a precisão usando a função `accuracy_score`. Essa função compara as previsões `y_pred_svm` com as respostas reais `y_test` para calcular a porcentagem de acertos.\n",
    "\n",
    "3. **Gerar e Imprimir o Relatório de Classificação**: Para uma análise mais detalhada, geramos um relatório de classificação com a função `classification_report`. Esse relatório mostra a precisão, o recall e a pontuação F1 para cada classe de sentimento (positivo, negativo). Isso nos ajuda a entender melhor as forças e fraquezas do modelo em classificar corretamente os sentimentos.\n",
    "\n",
    "Esse processo ajuda a avaliar a eficácia do modelo SVM em identificar corretamente os sentimentos nas avaliações, fornecendo insights valiosos para possíveis ajustes no modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prever os sentimentos no conjunto de teste\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Calcular a precisão\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Gerar e imprimir o relatório de classificação\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "O modelo SVM Linear, usando a técnica de bigrams no processo de BoW, foi o mais eficaz para a análise de sentimentos das avaliações do Uber. Essa abordagem alcançou uma precisão geral de 76.43% com resultados sólidos em todas as categorias de sentimentos. Continuaremos refinando o modelo para melhorar sua precisão e capacidade de generalização."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo ML + Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Divisão da base em treino e teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir os dados em conjuntos de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(embeddings_matrix, mapped_sentiments, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = SVC(kernel='linear',  decision_function_shape='ovo')\n",
    "svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
    "modelXgb = xgb.XGBClassifier(objective='multi:softmax', num_class=len(np.unique(mapped_sentiments)), **best_params)\n",
    "modelXgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo Regressão Logistica**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros para o modelo de Regressão Logística\n",
    "best_params = {'C': 1.0, 'solver': 'lbfgs', 'max_iter': 100}\n",
    "model_log_reg = LogisticRegression(**best_params)\n",
    "\n",
    "# Treinamento do modelo com X_train e y_train\n",
    "model_log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo Gradiente Boosting Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros para o modelo Gradient Boosting\n",
    "best_params = {'learning_rate': 0.2, 'n_estimators': 100, 'max_depth': 4}\n",
    "model_gbm = GradientBoostingClassifier(**best_params)\n",
    "\n",
    "# Treinamento do modelo com X_train e y_train\n",
    "model_gbm.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo - Rede neural**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando o modelo de rede neural\n",
    "model_nn = Sequential([\n",
    "    Dense(128, activation='relu', input_dim=X_train.shape[1]),  # Camada de entrada\n",
    "    Dense(64, activation='relu'),  # Camada oculta\n",
    "    Dense(1, activation='sigmoid')  # Camada de saída\n",
    "])\n",
    "\n",
    "# Compilando o modelo\n",
    "model_nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinamento do modelo\n",
    "model_nn.fit(X_train, y_train, epochs=10, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo LightGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurando parâmetros para LightGBM\n",
    "params = {\n",
    "    'boosting_type': 'gbdt',  # Tipo tradicional de gradient boosting decision tree\n",
    "    'objective': 'binary',    # Objetivo de classificação binária, mudar para 'multiclass' se necessário\n",
    "    'metric': 'binary_logloss',  # Métrica para avaliação de classificação binária\n",
    "    'num_leaves': 31,         # Número de folhas em uma árvore\n",
    "    'learning_rate': 0.05,    # Taxa de aprendizado\n",
    "    'feature_fraction': 0.9,  # Fração de características a serem selecionadas aleatoriamente para cada árvore\n",
    "    'bagging_fraction': 0.8,  # Fração de dados a serem usados em cada árvore\n",
    "    'bagging_freq': 5         # Frequência de bagging\n",
    "}\n",
    "\n",
    "# Criando o dataset de treinamento para LightGBM\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "\n",
    "# Treinamento do modelo\n",
    "model_lgbm = lgb.train(params, train_data, num_boost_round=200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelo LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros\n",
    "input_dim = X_train.shape[1]  # Número de features (dependendo de como você processou seus embeddings)\n",
    "output_dim = 25  # Dimensão de saída da camada Embedding\n",
    "input_length = 50  # Comprimento da entrada (número de palavras/tokens por exemplo)\n",
    "\n",
    "# Construindo o modelo\n",
    "model_lstm = Sequential([\n",
    "    Embedding(input_dim, output_dim, input_length=input_length),  # Camada de embedding para vetorizar\n",
    "    LSTM(32),  # Camada LSTM com 128 unidades\n",
    "    Dense(1, activation='sigmoid')  # Camada de saída para classificação binária\n",
    "])\n",
    "\n",
    "# Compilando o modelo\n",
    "model_lstm.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Treinamento do modelo\n",
    "model_lstm.fit(X_train, y_train, epochs=4, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado - Modelo SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prever os sentimentos no conjunto de teste\n",
    "y_pred = svm_model.predict(X_test)\n",
    "\n",
    "# Calcular a precisão\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Gerar e imprimir o relatório de classificação\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy:** 0.7026431718061674\n",
    "\n",
    "| Class | Precision | Recall | F1-Score | Support |\n",
    "|-------|------------|--------|----------|---------|\n",
    "| 0     | 0.79       | 0.80   | 0.80     | 254     |\n",
    "| 1     | 0.52       | 0.45   | 0.48     | 126     |\n",
    "| 2     | 0.68       | 0.78   | 0.73     | 74      |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado - Modelo XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prever os sentimentos no conjunto de teste\n",
    "y_pred = modelXgb.predict(X_test)\n",
    "\n",
    "# Calcular a precisão\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Gerar e imprimir o relatório de classificação\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.7731277533039648\n",
    "\n",
    "| Class | Precision | Recall | F1-Score | Support |\n",
    "|-------|------------|--------|----------|---------|\n",
    "| 0     | 0.84       | 0.79   | 0.82     | 254     |\n",
    "| 1     | 0.59       | 0.70   | 0.64     | 126     |\n",
    "| 2     | 0.94       | 0.84   | 0.89     | 74      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado - Modelo Regressão Logistica**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prever os sentimentos no conjunto de teste\n",
    "y_pred = model_log_reg.predict(X_test)\n",
    "\n",
    "# Calcular a precisão\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Gerar e imprimir o relatório de classificação\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.73568281938326\n",
    "\n",
    "| Class | Precision | Recall | F1-Score | Support |\n",
    "|-------|------------|--------|----------|---------|\n",
    "| 0     | 0.82       | 0.83   | 0.83     | 254     |\n",
    "| 1     | 0.58       | 0.56   | 0.57     | 126     |\n",
    "| 2     | 0.68       | 0.70   | 0.69     | 74      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado - Modelo GBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prever os sentimentos no conjunto de teste\n",
    "y_pred = model_gbm.predict(X_test)\n",
    "\n",
    "# Calcular a precisão\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Gerar e imprimir o relatório de classificação\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.7533039647577092\n",
    "\n",
    "| Class | Precision | Recall | F1-Score | Support |\n",
    "|-------|------------|--------|----------|---------|\n",
    "| 0     | 0.83       | 0.78   | 0.81     | 254     |\n",
    "| 1     | 0.55       | 0.69   | 0.61     | 126     |\n",
    "| 2     | 0.98       | 0.76   | 0.85     | 74      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado - Modelo Rede Neural**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prever os sentimentos no conjunto de teste\n",
    "y_pred = model_nn.predict(X_test)\n",
    "\n",
    "# Converter probabilidades em classes binárias baseadas em um limiar (0.5)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Calcular a precisão\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Gerar e imprimir o relatório de classificação\n",
    "print(classification_report(y_test, y_pred_classes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.6277533039647577\n",
    "\n",
    "| Class | Precision | Recall | F1-Score | Support |\n",
    "|-------|------------|--------|----------|---------|\n",
    "| 0     | 0.94       | 0.66   | 0.77     | 254     |\n",
    "| 1     | 0.43       | 0.94   | 0.59     | 126     |\n",
    "| 2     | 0.00       | 0.00   | 0.00     | 74      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado - Modelo LGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_lgbm.predict(X_test)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)  # Convertendo probabilidades para classes binárias\n",
    "\n",
    "# Avaliar o modelo\n",
    "accuracy = accuracy_score(y_test, y_pred_classes)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.6365638766519823\n",
    "\n",
    "| Class | Precision | Recall | F1-Score | Support |\n",
    "|-------|------------|--------|----------|---------|\n",
    "| 0     | 0.88       | 0.74   | 0.81     | 254     |\n",
    "| 1     | 0.42       | 0.79   | 0.55     | 126     |\n",
    "| 2     | 0.00       | 0.00   | 0.00     | 74      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resultado - Modelo LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previsão dos rótulos no conjunto de teste\n",
    "y_pred_probs = model_lstm.predict(X_test)\n",
    "y_pred = (y_pred_probs > 0.5).astype(int)\n",
    "\n",
    "# Cálculo das métricas\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Gerando e imprimindo o relatório de classificação\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy: 0.29955947136563876\n",
    "\n",
    "| Class | Precision | Recall | F1-Score | Support |\n",
    "|-------|------------|--------|----------|---------|\n",
    "| 0     | 0.86       | 0.05   | 0.09     | 254     |\n",
    "| 1     | 0.28       | 0.98   | 0.44     | 126     |\n",
    "| 2     | 0.00       | 0.00   | 0.00     | 74      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exportar Modelo - ML + Bert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para salvar o modelo, utilizamos dois métodos. Primeiro, salvamos os pesos do modelo em um arquivo .h5 usando model.save_weights. Em seguida, exportamos a arquitetura do modelo em formato JSON, salvando-a em um arquivo .json com model.to_json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = {'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 200}\n",
    "modelXgb = xgb.XGBClassifier(objective='multi:softmax', num_class=len(np.unique(mapped_sentiments)), **best_params)\n",
    "modelXgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvando o modelo treinado\n",
    "with open('trained_model_gbm.pkl', 'wb') as f:\n",
    "    pickle.dump(modelXgb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_test(text):\n",
    "    text = remove_urls(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    tokens = lemmatize_tokens_with_pos(tokens)\n",
    "    tokens = remove_punctuation_from_tokens(tokens)\n",
    "    tokens = remove_numbers_from_tokens(tokens)\n",
    "    tokens = remove_stopwords_preserve_adverbs(tokens)\n",
    "    tokens = filter_empty_tokens(tokens)\n",
    "    tokens = spell_checker(tokens)\n",
    "    return tokens\n",
    "\n",
    "# Exemplo de uso\n",
    "sample_text = \"i dont like this uber guuys\"\n",
    "pipeline_test(sample_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell_checker(\"I don't like uber\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# texto_processado = filter_empty_tokens(remove_stopwords(remove_numbers_from_tokens(remove_punctuation_from_tokens(lemmatize_tokens_with_pos(tokenize_text(remove_urls(texto)))))))\n",
    "pipeline_test(\"I very hates uber hates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_trained_model(model_path):\n",
    "    with open(model_path, 'rb') as model_file:\n",
    "        trained_model = pickle.load(model_file)\n",
    "    return trained_model\n",
    "\n",
    "def bert_embeddings(docs):\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-large-cased')\n",
    "    model_bert = BertModel.from_pretrained('bert-large-cased')\n",
    "    embeddings = []\n",
    "    for doc in docs:\n",
    "        inputs = tokenizer(doc, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        outputs = model_bert(**inputs)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :].detach().numpy()\n",
    "        embeddings.append(cls_embedding)\n",
    "    embeddings_matrix = np.vstack(embeddings)\n",
    "    return embeddings_matrix\n",
    "\n",
    "def fazer_predicao(modelo, texto):\n",
    "    texto_processado = pipeline_test(texto)\n",
    "    embeddings_texto = bert_embeddings([texto_processado])\n",
    "    print(texto_processado)\n",
    "    predicoes = modelo.predict(embeddings_texto)\n",
    "    return predicoes\n",
    "\n",
    "# Carregar o modelo treinado\n",
    "trained_model_path = 'trained_model_gbm.pkl'\n",
    "trained_model = load_trained_model(trained_model_path)\n",
    "\n",
    "# Fazer uma predição\n",
    "texto = \"Wow, @Uber never ceases to impress. Needed a quick ride to the airport and they delivered. Fast and reliable as always! #Impressed\"\n",
    "predicted_class = fazer_predicao(trained_model, texto)\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exportar Modelo - Bigrams + ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_test(text):\n",
    "    text = remove_urls(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    tokens = lemmatize_tokens_with_pos(tokens)\n",
    "    tokens = remove_punctuation_from_tokens(tokens)\n",
    "    tokens = remove_numbers_from_tokens(tokens)\n",
    "    tokens = remove_stopwords_preserve_adverbs(tokens)\n",
    "    tokens = filter_empty_tokens(tokens)\n",
    "    tokens = spell_checker(tokens)\n",
    "    return tokens\n",
    "\n",
    "# Exemplo de uso\n",
    "sample_text = \"i dont like this uber guuys\"\n",
    "pipeline_test(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar os dados\n",
    "df = pd.read_csv('../Notebooks/pipeline_comments.csv')\n",
    "\n",
    "# Função para vetorização usando bigramas\n",
    "def sklearn_bow_bigrams(docs):\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 4))\n",
    "    bow_matrix = vectorizer.fit_transform(docs)\n",
    "    return bow_matrix, vectorizer\n",
    "\n",
    "# Gerar matriz BoW usando a coluna 'comment'\n",
    "X, vectorizer = sklearn_bow_bigrams(df['comment'])\n",
    "\n",
    "# Dividir o dataset em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Treinar o modelo SVM\n",
    "model = SVC(kernel='linear', decision_function_shape='ovo')\n",
    "model.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar o modelo e o vetorizador em um arquivo pickle\n",
    "with open('svm_model.pkl', 'wb') as file:\n",
    "    pickle.dump(model, file)\n",
    "\n",
    "with open('vectorizer.pkl', 'wb') as file:\n",
    "    pickle.dump(vectorizer, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o modelo e o vetorizador do arquivo pickle\n",
    "with open('svm_model.pkl', 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "with open('vectorizer.pkl', 'rb') as file:\n",
    "    loaded_vectorizer = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suponha que temos um novo comentário para previsão\n",
    "new_comment = \"Felt very unsafe in my @Uber ride tonight. The driver was driving recklessly. Not what I expected from a top-rated service. #safetyfirst\"\n",
    "\n",
    "texto_processado = pipeline_test(new_comment)\n",
    "\n",
    "print(texto_processado)\n",
    "\n",
    "# Vetorizar o novo comentário usando o vetorizador carregado\n",
    "# Convertendo a string em uma lista antes de vetorizar\n",
    "new_comment_vectorized = loaded_vectorizer.transform([texto_processado])\n",
    "\n",
    "# Usar o modelo carregado para fazer previsões\n",
    "prediction = loaded_model.predict(new_comment_vectorized)\n",
    "\n",
    "# Exibir a previsão\n",
    "print(f\"Previsão para o comentário: {prediction[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supondo que 'model' é o seu modelo treinado e 'X_test', 'y_test' são seus dados de teste\n",
    "\n",
    "# Prever os sentimentos no conjunto de teste\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calcular a precisão\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Gerar e imprimir o relatório de classificação\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Rótulos reais usados no dataset\n",
    "labels = [-1, 0, 1]\n",
    "\n",
    "# Criar a matriz de confusão com rótulos específicos\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "\n",
    "# Plotar a matriz de confusão usando Seaborn\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', cbar=False, xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
