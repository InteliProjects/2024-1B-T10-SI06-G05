{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook de Exploração (word2vec)\n",
    "\n",
    "Nesse arquivo, realizamos uma análise detalhada e experimentação com diversos modelos de aprendizado de máquina para determinar qual oferece o melhor desempenho para a nossa tarefa específica. Este processo inclui a preparação e a limpeza dos dados, o balanceamento das classes, a extração de características relevantes e a avaliação de múltiplos algoritmos. Diversos testes e validações são conduzidos para refinar os modelos e otimizar seus parâmetros. O objetivo final é identificar o modelo mais eficaz e robusto para ser utilizado em produção, garantindo uma análise precisa e confiável dos dados."
   ]
  },
{
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justificativa para a Escolha do Modelo Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este projeto, optamos por utilizar o modelo Word2Vec pré-treinado do Google. A principal razão é que nossa base de dados da Uber está em inglês, e o modelo do Google foi treinado em um grande corpus de texto em inglês, garantindo uma melhor representação semântica das palavras em nosso contexto. Embora existam outras alternativas, como o Word2Vec do NILC treinado em português, a escolha do modelo do Google é mais adequada para nossa necessidade específica, pois oferece uma vasta quantidade de dados em inglês, apesar de ter menos alternativas de modelos com dimensões diferentes."
   ]
  },

  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baixar Pandas para importar CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../Notebooks/dados.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Exploratória do Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Introdução**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A análise exploratória de dados é um passo fundamental para desenvolver modelos que classificam sentimentos, especialmente quando lidamos com dados complexos como comentários de usuários. Neste projeto, o grupo Moodfy estudou um conjunto de comentários sobre a Uber retirados da plataforma X (antes conhecida como Twitter). O objetivo dessa análise inicial era entender melhor os dados, descobrir padrões e preparar a base para criar um modelo que possa classificar os comentários como positivos, negativos ou neutros. Esse trabalho inicial é crucial para garantir que o modelo final seja preciso e eficaz, influenciando decisões importantes sobre como atender e se comunicar com os clientes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explicação dos Passos da Análise Exploratória Feita pelo Grupo Moodfy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Distribuição de Sentimentos: O grupo Moodfy começou analisando como os sentimentos estavam distribuídos entre os comentários para ver se havia um equilíbrio entre positivos, negativos e neutros. Essa checagem é importante porque um desequilíbrio pode fazer o modelo futuro pender para o lado mais comum.\n",
    "\n",
    "2. Análise de Palavras Comuns em Comentários: Eles identificaram quais palavras apareciam mais em cada tipo de sentimento. Esse passo ajuda a entender quais temas são frequentes e como certas palavras podem influenciar a classificação dos sentimentos.\n",
    "\n",
    "3. Análise de Comprimento dos Comentários: O grupo investigou se o tamanho dos comentários estava relacionado com os sentimentos expressados. Comentários mais longos podem indicar sentimentos mais fortes.\n",
    "\n",
    "4. Frequência de Palavras por Sentimento: Essa etapa detalhou mais a análise anterior, quantificando quantas vezes certas palavras apareciam nos diferentes sentimentos. Isso foi útil para ver se algumas palavras muito comuns, que não adicionam muito significado, estavam influenciando os resultados.\n",
    "\n",
    "5. Correlação entre Comprimento do Comentário e Sentimento: Eles também verificaram se havia uma relação entre o tamanho dos comentários e o tipo de sentimento, para entender se pessoas mais satisfeitas ou insatisfeitas tendem a escrever mais.\n",
    "\n",
    "6. Média de Comprimento dos Comentários por Sentimento: Por último, o grupo calculou o comprimento médio dos comentários para cada sentimento, buscando identificar comentários muito longos ou curtos que fogem do comum, o que pode sugerir a necessidade de ajustar os dados antes de criar o modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importação de bibliotecas de análise gráfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise Exploratória do Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criação de Dataframe exclusivo para análise exploratória do corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = pd.read_csv('../Notebooks/dados.csv')\n",
    "\n",
    "df_analysis.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Distribuição de Sentimentos**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este gráfico de barras mostra a frequência de cada categoria de sentimento nos dados. Os sentimentos estão categorizados como -1 (negativo), 0 (neutro) e 1 (positivo). O gráfico ajuda a visualizar rapidamente a proporção de comentários em cada categoria, permitindo uma análise quantitativa rápida da natureza geral dos comentários no dataset. A visualização utiliza cores diferentes para cada categoria para facilitar a distinção: vermelho para negativo, cinza para neutro e azul para positivo. Esta análise é baseada no autoestudo \"Como Fazer Análise de Sentimentos com Dados Textuais\", onde aprendemos a aplicar métodos estatísticos para entender a distribuição de sentimentos em dados textuais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sentiment_distribution(df_analysis):\n",
    "    \"\"\"\n",
    "    Cria um gráfico de barras para mostrar a distribuição dos sentimentos nos comentários.\n",
    "    \n",
    "    Args:\n",
    "        df_analysis (DataFrame): DataFrame contendo os dados dos sentimentos.\n",
    "    \n",
    "    Returns:\n",
    "        None: Exibe o gráfico de barras com a distribuição dos sentimentos.\n",
    "    \"\"\"\n",
    "    # Calcula a contagem de cada sentimento\n",
    "    sentiment_counts = df_analysis['sentiment'].value_counts().sort_index()\n",
    "\n",
    "    # Gráfico de barras\n",
    "    plt.bar(sentiment_counts.index, sentiment_counts.values, color=['red', 'grey', 'blue'])\n",
    "    plt.xlabel('Sentimento')\n",
    "    plt.ylabel('Frequência de comentários')\n",
    "    plt.title('Distribuição de Sentimentos')\n",
    "    plt.xticks([-1, 0, 1], ['Negativo', 'Neutro', 'Positivo'])\n",
    "    plt.show()\n",
    "\n",
    "plot_sentiment_distribution(df_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Análise de Palavras Comuns em Comentários**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando a técnica de nuvem de palavras, este gráfico destaca as palavras mais comuns em comentários de diferentes sentimentos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_wordcloud(data, title):\n",
    "    \"\"\"Gera uma nuvem de palavras para visualizar as palavras mais comuns em comentários.\n",
    "\n",
    "    Args:\n",
    "        data (Series): Série de Pandas contendo os comentários.\n",
    "        title (str): Título para a nuvem de palavras.\n",
    "\n",
    "    Returns:\n",
    "        None: Exibe a nuvem de palavras.\n",
    "    \"\"\"\n",
    "    wc = WordCloud(background_color='white', max_words=200)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(wc.generate(' '.join(data)), interpolation='bilinear')\n",
    "    plt.title(title)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Gerar Word Clouds para cada sentimento\n",
    "generate_wordcloud(df_analysis[df_analysis['sentiment'] == -1]['comment'], 'Palavras mais comuns em comentários negativos')\n",
    "generate_wordcloud(df_analysis[df_analysis['sentiment'] == 0]['comment'], 'Palavras mais comuns em comentários neutros')\n",
    "generate_wordcloud(df_analysis[df_analysis['sentiment'] == 1]['comment'], 'Palavras mais comuns em comentários positivos')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Análise de Comprimento dos Comentários**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este gráfico de caixa (boxplot) ilustra a distribuição do comprimento dos comentários com base nos sentimentos expressos, categorizados em negativo (-1), neutro (0) e positivo (1). Cada caixa no gráfico representa a distribuição dos comprimentos de comentários para uma categoria de sentimento específica, oferecendo uma visão sobre a mediana, os quartis e os valores extremos (outliers) de comprimento para cada grupo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comment_length_by_sentiment(df_analysis):\n",
    "    \"\"\"Gera um gráfico de caixa para visualizar a distribuição do comprimento dos comentários por sentimento.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame contendo as colunas 'comment' e 'sentiment', onde 'comment' são os comentários\n",
    "                        e 'sentiment' são os sentimentos associados aos comentários.\n",
    "\n",
    "    Returns:\n",
    "        None: Exibe o gráfico de caixa mostrando o comprimento dos comentários distribuídos por sentimentos.\n",
    "    \"\"\"\n",
    "    # Calcula o comprimento de cada comentário\n",
    "    df_analysis['comment_length'] = df_analysis['comment'].apply(len)\n",
    "\n",
    "    # Configura e exibe o gráfico de caixa\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='sentiment', y='comment_length', data=df_analysis)\n",
    "    plt.title('Comprimento dos Comentários por Sentimento')\n",
    "    plt.xlabel('Sentimento')\n",
    "    plt.ylabel('Comprimento do Comentário (N° de caraceteres)')\n",
    "    plt.xticks(ticks=[0, 1, 2], labels=['Negativo (-1)', 'Neutro (0)', 'Positivo (1)'])\n",
    "    plt.show()\n",
    "\n",
    "# chamada da função:\n",
    "plot_comment_length_by_sentiment(df_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Frequência de Palavras por Sentimento**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este gráfico de barras compara a frequência das palavras mais comuns em comentários negativos e neutros. As barras indicam quantas vezes cada palavra foi mencionada, proporcionando uma visualização clara das diferenças no vocabulário usado em diferentes estados emocionais. Analisar essas frequências ajuda a entender os temas predominantes e possíveis áreas de insatisfação ou discussões gerais que não envolvem sentimentos fortes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_word_frequency_pipeline(df):\n",
    "    \"\"\"Analisa e visualiza as palavras mais comuns em comentários categorizados por sentimentos negativos, neutros e positivos.\n",
    "\n",
    "    Args:\n",
    "        df_analysis (DataFrame): DataFrame contendo as colunas 'comment' e 'sentiment', onde 'comment' são os comentários\n",
    "                        e 'sentiment' identifica o sentimento do comentário.\n",
    "\n",
    "    Returns:\n",
    "        None: Gera gráficos de barras mostrando as palavras mais comuns para cada categoria de sentimento.\n",
    "    \"\"\"\n",
    "    # Função para contar palavras em um texto\n",
    "    def count_words(text):\n",
    "        words = text.split()\n",
    "        return Counter(words)\n",
    "\n",
    "    # Aplicação da função de contagem de palavras e agregação por sentimento\n",
    "    df['words'] = df['comment'].apply(count_words)\n",
    "    negative_words = sum(df[df['sentiment'] == -1]['words'], Counter())\n",
    "    neutral_words = sum(df[df['sentiment'] == 0]['words'], Counter())\n",
    "    positive_words = sum(df[df['sentiment'] == 1]['words'], Counter())\n",
    "\n",
    "    # Seleção das 10 palavras mais comuns em cada categoria de sentimento\n",
    "    most_common_neg = negative_words.most_common(10)\n",
    "    most_common_neu = neutral_words.most_common(10)\n",
    "    most_common_pos = positive_words.most_common(10)\n",
    "\n",
    "    # Criação de gráficos de barras para cada categoria de sentimento\n",
    "    fig, ax = plt.subplots(3, 1, figsize=(10, 8))\n",
    "\n",
    "    ax[0].bar([word[0] for word in most_common_neg], [word[1] for word in most_common_neg], color='red')\n",
    "    ax[0].set_title('Palavras mais comuns em comentários negativos')\n",
    "    ax[0].set_ylabel('Frequência De Repetição da Palavra')\n",
    "\n",
    "    ax[1].bar([word[0] for word in most_common_neu], [word[1] for word in most_common_neu], color='grey')\n",
    "    ax[1].set_title('Palavras mais comuns em comentários neutros')\n",
    "    ax[1].set_ylabel('Frequência De Repetição da Palavra')\n",
    "\n",
    "    ax[2].bar([word[0] for word in most_common_pos], [word[1] for word in most_common_pos], color='green')\n",
    "    ax[2].set_title('Palavras mais comuns em comentários positivos')\n",
    "    ax[2].set_ylabel('Frequência De Repetição da Palavra')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# chamada da função:\n",
    "analyze_word_frequency_pipeline(df_analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. Correlação entre Comprimento do Comentário e Sentimento**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este gráfico de dispersão explora se o comprimento dos comentários varia com o sentimento expresso. Cada ponto representa um comentário, posicionado de acordo com seu sentimento (negativo, neutro) e comprimento. A visualização ajuda a identificar se comentários mais longos tendem a ser negativos, positivos, ou neutros, fornecendo insights sobre como os usuários se expressam em diferentes contextos emocionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comment_length_sentiment_correlation(df_analysis):\n",
    "    \"\"\"Gera um gráfico de dispersão para examinar a correlação entre o comprimento dos comentários e os sentimentos expressos.\n",
    "\n",
    "    Args:\n",
    "        df_analysis (DataFrame): DataFrame contendo as colunas 'sentiment' e 'comment_length', onde 'sentiment' indica o sentimento\n",
    "                        do comentário e 'comment_length' é o comprimento do comentário medido em número de caracteres.\n",
    "\n",
    "    Returns:\n",
    "        None: Exibe o gráfico de dispersão.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.scatter(df_analysis['sentiment'], df_analysis['comment_length'], alpha=0.5)\n",
    "    plt.title('Correlação entre Comprimento do Comentário e Sentimento')\n",
    "    plt.xlabel('Tipo de Sentimento')\n",
    "    plt.ylabel('Comprimento do Comentário - (N° Caracteres)')\n",
    "    plt.xticks([-1, 0, 1], ['Negativo', 'Neutro', 'Positivo'])\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# chamada da função:\n",
    "plot_comment_length_sentiment_correlation(df_analysis)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Média de Comprimento dos Comentários por Sentimento**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse é um gráfico de barras que mostra a média de comprimento dos comentários para cada categoria de sentimento. Este gráfico pode indicar se sentimentos mais fortes (positivos ou negativos) levam a comentários mais detalhados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_average_comment_length_by_sentiment(df_analysis):\n",
    "    \"\"\"Calcula e visualiza a média de comprimento dos comentários por sentimento em um gráfico de barras.\n",
    "\n",
    "    Args:\n",
    "        df_analysis (DataFrame): DataFrame contendo as colunas 'sentiment' e 'comment_length', onde 'sentiment' indica o sentimento\n",
    "                        do comentário e 'comment_length' é o comprimento do comentário.\n",
    "\n",
    "    Returns:\n",
    "        None: Exibe o gráfico de barras mostrando a média de comprimento dos comentários para cada sentimento.\n",
    "    \"\"\"\n",
    "    # Cálculo da média de comprimento dos comentários por sentimento\n",
    "    average_lengths = df_analysis.groupby('sentiment')['comment_length'].mean()\n",
    "\n",
    "    # Configuração e exibição do gráfico de barras\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.barplot(x=average_lengths.index, y=average_lengths.values, palette='coolwarm')\n",
    "    plt.title('Média de Comprimento dos Comentários por Sentimento')\n",
    "    plt.xlabel('Sentimento')\n",
    "    plt.ylabel('Média de Comprimento do Comentário (N° de Caracteres)')\n",
    "    plt.xticks(ticks=[0, 1, 2], labels=['Negativo (-1)', 'Neutro (0)', 'Positivo (1)'])\n",
    "    plt.show()\n",
    "\n",
    "# chamada da função:\n",
    "plot_average_comment_length_by_sentiment(df_analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-Processamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O pré-processamento é uma etapa fundamental em um modelo de machine learning, responsável por preparar e organizar os dados antes de serem alimentados ao algoritmo de aprendizado. Ele inclui uma série de técnicas e procedimentos, como normalização, padronização, tratamento de dados faltantes e seleção de características relevantes. A funcionalidade do pré-processamento é melhorar a qualidade dos dados, tornando-os mais adequados e representativos para o modelo de machine learning. Isso ajuda a evitar problemas como overfitting, garantir que o modelo possa generalizar bem para dados novos e melhorar a eficácia do aprendizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from spellchecker import SpellChecker\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from gensim.models import KeyedVectors, FastText, Word2Vec\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Baixar os recursos necessários do NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenização é o processo de dividir um texto em unidades menores chamadas tokens. Esses tokens podem ser palavras individuais, partes de palavras ou até mesmo caracteres, dependendo do nível de granularidade desejado. A tokenização é uma etapa fundamental no processamento de linguagem natural (PLN), sendo essencial para a preparação, análise e manipulação de texto em uma variedade de aplicações, incluindo análise de sentimento, classificação de texto e tradução automática. Ao dividir o texto em tokens, os dados tornam-se estruturados e adequados para análise, facilitando a extração de informações e a modelagem de soluções de PLN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokeniza o DataFrame em uma lista de tokens.\n",
    "\n",
    "    Args:\n",
    "    text (DataFrame): Comentários a serem tokenizados.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de tokens.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lemmatização é o processo de reduzir palavras a sua forma base ou lema, considerando o contexto e a morfologia da língua. Essa técnica é importante em PLN para tratar diferentes formas de uma palavra como iguais, como \"corre\" e \"correu\" ambas reduzidas a \"correr\". Isso simplifica o processamento de texto e melhora a análise em tarefas como recuperação de informações e análise de sentimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Retorna o tag correspondente do WordNet para o tag do Treebank do Penn.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN \n",
    "\n",
    "def lemmatize_tokens_with_pos(tokens):\n",
    "    \"\"\"\n",
    "    Lemmatiza uma lista de tokens baseando-se em sua parte do discurso.\n",
    "\n",
    "    Args:\n",
    "    tokens (list of str): Tokens a serem lematizados.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de lemas das palavras.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    # Tokeniza e aplica POS tagging\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "\n",
    "    # Lemmatiza usando a parte do discurso\n",
    "    lemmas = [lemmatizer.lemmatize(token, get_wordnet_pos(pos)) for token, pos in tagged_tokens]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retirar pontuações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A remoção de pontuação é um processo utilizado no pré-processamento de texto que visa eliminar caracteres de pontuação, como vírgulas, pontos e pontos de exclamação, de um texto. Isso é feito para limpar o texto e reduzir a dimensionalidade dos dados, facilitando a análise e a modelagem. Ao remover a pontuação, os tokens resultantes contêm apenas palavras ou partes de palavras, tornando-os mais adequados para tarefas de processamento de linguagem natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_from_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Remove pontuações de uma lista de tokens e exclui os tokens que consistem exclusivamente de caracteres de pontuação.\n",
    "\n",
    "    Args:\n",
    "    tokens (list): Lista de tokens a serem processados.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de tokens sem pontuações.\n",
    "    \"\"\"\n",
    "    # Regex para identificar pontuações\n",
    "    regex_punctuation = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "    # Remove pontuações de cada token e filtra tokens que ficaram vazios ou são apenas pontuações\n",
    "    tokens_no_punct = [regex_punctuation.sub('', token) for token in tokens]\n",
    "    tokens_no_punct = [token for token in tokens_no_punct if token.strip() != '']\n",
    "\n",
    "    return tokens_no_punct\n",
    "\n",
    "# Exemplo de uso da função:\n",
    "tokens = [\"hello!\", \"world...\", \"#amazing\", \"test,\", \":)\", \"a\", \".\"]\n",
    "filtered_tokens = remove_punctuation_from_tokens(tokens)\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retirar números"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A remoção de números é um passo comum no pré-processamento de texto que consiste em eliminar todos os caracteres numéricos de um texto. Isso é feito para limpar o texto de informações numéricas que podem não ser relevantes para a análise ou para garantir que as palavras sejam tratadas de maneira uniforme durante a tokenização. Ao remover números, os tokens resultantes contêm apenas palavras e outros caracteres não numéricos, simplificando o texto para análise e modelagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers_from_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Remove todos os dígitos de uma lista de tokens e remove os tokens que consistem exclusivamente de números.\n",
    "\n",
    "    Args:\n",
    "    tokens (list): Lista de tokens a serem processados.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de tokens sem números.\n",
    "    \"\"\"\n",
    "    # Remover dígitos de cada token e depois filtrar os tokens que são apenas números ou ficaram vazios\n",
    "    tokens_no_numbers = [re.sub(r'\\d+', '', token) for token in tokens]\n",
    "    tokens_no_numbers = [token for token in tokens_no_numbers if token.strip() != '']\n",
    "    return tokens_no_numbers\n",
    "\n",
    "# Exemplo de uso da função:\n",
    "tokens = [\"hello123\", \"world\", \"2023\", \"test\", \"12345\"]\n",
    "filtered_tokens = remove_numbers_from_tokens(tokens)\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção de Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords são palavras que são frequentemente removidas durante o pré-processamento de texto em tarefas de Processamento de Linguagem Natural (PLN). Essas palavras são geralmente as mais comuns em um idioma, como ‘é’, ‘em’, ‘um’, ‘e’ em português, ou ‘is’, ‘in’, ‘a’, ‘and’ em inglês, e tendem a aparecer em quase todos os documentos de um corpus.\n",
    "\n",
    "A remoção de stopwords é uma prática comum porque essas palavras, embora muito frequentes, geralmente não carregam muito significado e podem adicionar ruído aos dados. Além disso, removendo-as, podemos reduzir o tamanho do nosso vocabulário e, consequentemente, o espaço de recursos, tornando nossos modelos de PLN mais eficientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopwordRemover(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Uma classe transformadora para remover stopwords e aplicar transformações de texto específicas,\n",
    "    incluindo a remoção de aspas simples e duplas, e a transformação de 's em is.\n",
    "    \n",
    "    Args:\n",
    "    language (str): Idioma das stopwords a serem removidas.\n",
    "    \n",
    "    Returns:\n",
    "    list: Lista de comentários com stopwords removidas e transformações de texto aplicadas.\n",
    "    \"\"\"\n",
    "    def __init__(self, language='english'):\n",
    "        self.stopwords = set(stopwords.words(language))\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        processed = []\n",
    "        for comment in X:\n",
    "            # Remover aspas simples e duplas\n",
    "            comment = re.sub(r\"\\'\", \"\", comment)  # Remove todas as aspas simples\n",
    "            comment = re.sub(r\"\\\"\", \"\", comment)  # Remove todas as aspas duplas\n",
    "\n",
    "            # Substituir 's por is (considerando casos como it's, he's etc.)\n",
    "            comment = re.sub(r\"\\'s\\b\", \"is\", comment)  # Substitui 's por is\n",
    "            \n",
    "            # Tokenização e remoção de stopwords\n",
    "            tokens = word_tokenize(comment)\n",
    "            filtered_tokens = [word for word in tokens if word.lower() not in self.stopwords]\n",
    "            \n",
    "            # Juntar os tokens filtrados de volta em uma string\n",
    "            processed_comment = ' '.join(filtered_tokens)\n",
    "            processed.append(processed_comment)\n",
    "        \n",
    "        return processed\n",
    "\n",
    "# Exemplo de uso da classe:\n",
    "X = [\"It's a good day!\", \"He didn't like the movie's plot.\", \"\\\"That's great!\\\", he said.\"]\n",
    "stopword_remover = StopwordRemover()\n",
    "X_transformed = stopword_remover.transform(X)\n",
    "print(X_transformed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remoção de links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "A remoção de links é uma etapa de pré-processamento em tarefas de Processamento de Linguagem Natural (PLN) que envolve a eliminação de URLs ou links de um texto. Isso é feito para reduzir o ruído nos dados e focar nas palavras e frases que carregam mais significado.\n",
    "\n",
    "Links geralmente não contribuem para a semântica de um texto e podem ser bastante variados e únicos, o que pode adicionar ruído e aumentar a dimensionalidade dos dados. Além disso, os links podem levar a conteúdo externo que está fora do contexto do texto atual, tornando a análise mais complexa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a função para remover URLs\n",
    "def remove_urls(text):\n",
    "    \"\"\"\n",
    "    Remove URLs de um texto fornecido.\n",
    "\n",
    "    Args:\n",
    "    texto (list): O texto de entrada contendo URLs.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de tokens com URLs removidas.\n",
    "    \"\"\"\n",
    "    return re.sub(r'http\\S+|www.\\S+', '', text, flags=re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balanceamento das Classes de Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O balanceamento das classes de dados é uma prática crucial em aprendizado de máquina para garantir que cada categoria seja representada de forma equitativa. Isso previne que o modelo desenvolva viés em direção à classe majoritária e melhora a precisão geral em prever categorias menos representadas. \n",
    "\n",
    "Na base de dados em questão, realizamos um balanceamento focado nas dinâmicas de desproporção entre as classes de sentimentos negativos e positivos. Primeiro, removemos os outliers baseando-nos apenas na distribuição dos próprios dados negativos para eliminar comentários atípicos que poderiam distorcer a análise. Após essa filtragem, procedemos com uma redução estratégica, eliminando aleatoriamente 40% dos dados negativos restantes. Além disso, para tratar a sub-representação dos sentimentos positivos, duplicamos os dados dessa classe. Essa abordagem combinada não só equilibra a presença das classes no dataset mas também facilita o treinamento de modelos mais justos e eficazes, evitando o viés em direção a qualquer classe específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_and_sample_negatives_and_duplicate_positives(df):\n",
    "    \"\"\"\n",
    "    Remove outliers do comprimento dos comentários para a classe de sentimentos negativos,\n",
    "    baseando-se nos quartis da própria classe negativa, e remove aleatoriamente 40% dos dados negativos\n",
    "    restantes para ajudar no balanceamento das classes. Adicionalmente, duplica os dados da classe positiva\n",
    "    para melhor balanceamento.\n",
    "\n",
    "    Args:\n",
    "    df (DataFrame): DataFrame contendo as colunas 'sentiment', 'comment', e 'comment_length'.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame com outliers removidos da classe negativa, redução estratégica de 40% dos negativos,\n",
    "    e duplicação dos dados positivos.\n",
    "    \"\"\"\n",
    "    # Adiciona a coluna 'comment_length' ao DataFrame se não existir\n",
    "    if 'comment_length' not in df.columns:\n",
    "        df['comment_length'] = df['comment'].apply(len)\n",
    "    \n",
    "    # Filtra os dados para a classe negativa\n",
    "    negativos = df[df['sentiment'] == -1]\n",
    "    \n",
    "    # Calcula os quartis apenas para a classe negativa\n",
    "    Q1 = negativos['comment_length'].quantile(0.25)\n",
    "    Q3 = negativos['comment_length'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Filtra os outliers na classe negativa baseado no limite calculado\n",
    "    negativos_filtrados = negativos[negativos['comment_length'] <= upper_bound]\n",
    "    \n",
    "    # Amostragem aleatória para remover 40% dos dados negativos filtrados\n",
    "    negativos_reduzidos = negativos_filtrados.sample(frac=0.6, random_state=42)\n",
    "\n",
    "    # Duplica os dados da classe positiva\n",
    "    positivos = df[df['sentiment'] == 1]\n",
    "    positivos_duplicados = pd.concat([positivos] * 2, ignore_index=True)\n",
    "    \n",
    "    # Combina os dados reduzidos de sentimentos negativos com as outras classes\n",
    "    df_final = pd.concat([negativos_reduzidos, df[df['sentiment'] == 0], positivos_duplicados], ignore_index=True)\n",
    "    \n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline é um conceito essencial em diversos campos, incluindo tecnologia da informação, manufatura, logística e até mesmo em processos criativos. Em termos gerais, refere-se a uma sequência de etapas interconectadas e ordenadas, onde o resultado de cada etapa serve como entrada para a próxima. Essas etapas podem ser atividades, operações ou processos específicos. O objetivo principal de um pipeline é otimizar a eficiência e a produtividade, dividindo um trabalho complexo em partes menores e mais gerenciáveis, permitindo que cada etapa seja executada de forma independente e simultânea. Isso não apenas acelera o processo, mas também permite melhorias contínuas em cada etapa individualmente, resultando em um produto final de maior qualidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o pipeline para pré-processamento dos comentários\n",
    "pipeline = Pipeline([\n",
    "    ('url_remover', FunctionTransformer(lambda x: x.apply(remove_urls))),\n",
    "    ('tokenizer', FunctionTransformer(lambda x: x.apply(tokenize_text))),\n",
    "    ('lemmatizacao', FunctionTransformer(lambda x: x.apply(lemmatize_tokens_with_pos))),\n",
    "    ('punctuation_remover', FunctionTransformer(lambda x: x.apply(remove_punctuation_from_tokens))),\n",
    "    ('number_remover', FunctionTransformer(lambda x: x.apply(remove_numbers_from_tokens))),\n",
    "    ('stopwords_remover', FunctionTransformer(lambda x: x.apply(StopwordRemover().transform))),\n",
    "    ('join_tokens', FunctionTransformer(lambda x: x.apply(remove_stopwords))),\n",
    "])\n",
    "\n",
    "# Primeiro aplicar a remoção de outliers e a redução de dados negativos\n",
    "df = remove_outliers_and_sample_negatives_and_duplicate_positives(df)\n",
    "\n",
    "# Aplicar o pipeline ao DataFrame existente\n",
    "df['comment'] = pipeline.fit_transform(df['comment'])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exportar CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('pipeline_comments.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise dos Corpus Após o Pré-Processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sentiment_distribution(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_wordcloud(df[df['sentiment'] == -1]['comment'], 'Palavras mais comuns em comentários negativos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comment_length_by_sentiment(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_word_frequency_pipeline(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comment_length_sentiment_correlation(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_average_comment_length_by_sentiment(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vetores pré-treinados\n",
    "\n",
    "Uso de vetores pré-treinados como ponto de partida para o treinamento do modelo Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram_pretrained = \"../src/GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "# Carregar modelo pré-treinado\n",
    "modelo_skipgram = KeyedVectors.load_word2vec_format(skipgram_pretrained, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste com vetores pré-treinados\n",
    "\n",
    "Testando a similaridade entre duas palavras usando o modelo pré-treinado para avaliar sua utilidade antes de proceder com o treinamento do nosso próprio modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec_word1 = modelo_skipgram['dog']\n",
    "wordvec_word2 = modelo_skipgram['puppy']\n",
    "\n",
    "# Calculando a similaridade\n",
    "similarity = 1 - cosine(wordvec_word1, wordvec_word2)\n",
    "print(f\"A similaridade entre 'dog' e 'puppy' é: {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento do modelo Skip-gram com nosso corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['comment'].apply(lambda x: word_tokenize(x.lower()) if isinstance(x, str) else [])\n",
    "\n",
    "sentences = corpus.tolist()\n",
    "\n",
    "# Treinar o modelo Word2Vec Skip-gram\n",
    "model_sg = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1, workers=4, epochs=10)\n",
    "\n",
    "# Salvar o modelo treinado\n",
    "model_sg.wv.save_word2vec_format('word2vec_skipgram_model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculando e exibindo similaridade com o modelo treinado\n",
    "\n",
    "Verificando a similaridade entre duas palavras para entender como o modelo está performando com os dados do projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavra1 = 'like'\n",
    "palavra2 = 'hate'\n",
    "\n",
    "similaridade_sg = model_sg.wv.similarity(palavra1, palavra2)\n",
    "print(f'A similaridade entre \"{palavra1}\" e \"{palavra2}\" é: {similaridade_sg}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função para somar os vetores de palavras da base\n",
    "\n",
    "Como nossa base é pautada em frases e não em palavras soltas, para que o modelo word2ved faça mais sentido, nós devemos fazer uma função para somar as palavras presentes em determinada frase e assim conseguir um vetor resultade da frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o modelo treinado\n",
    "model_sg = KeyedVectors.load_word2vec_format('word2vec_skipgram_model.bin', binary=True, unicode_errors='ignore')\n",
    "\n",
    "# Função para sumarizar vetores de palavras\n",
    "def sum_word_vectors_sg(sentence):\n",
    "    sentence_vector = np.zeros(model_sg.vector_size)\n",
    "    words_count = 0\n",
    "\n",
    "    for word in sentence:\n",
    "        if word in model_sg:\n",
    "            sentence_vector += model_sg[word]\n",
    "            words_count += 1\n",
    "\n",
    "    if words_count != 0:\n",
    "        sentence_vector /= words_count\n",
    "\n",
    "    return sentence_vector\n",
    "\n",
    "# Calcular e imprimir os vetores das sentenças\n",
    "sentences_vectors_sg = [sum_word_vectors_sg(sentence) for sentence in sentences]\n",
    "for i, vector in enumerate(sentences_vectors_sg):\n",
    "    print(f\"Vetor da Sentença {i+1}: {vector}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vetores pré-treinados\n",
    "\n",
    "Uso de vetores pré-treinados como ponto de partida para o treinamento do modelo CBOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = \"../src/GoogleNews-vectors-negative300.bin\"\n",
    "\n",
    "modelo_cbow = KeyedVectors.load_word2vec_format(cbow, binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Testes\n",
    "\n",
    "Testando a similaridade entre duas palavras usando o modelo pré-treinado para avaliar sua utilidade antes de proceder com o treinamento do nosso próprio modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec_word1 = modelo_cbow['dog']\n",
    "\n",
    "wordvec_word1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordvec_word2 = modelo_cbow['puppy']\n",
    "\n",
    "wordvec_word2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 - cosine(wordvec_word1, wordvec_word2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vetores da nossa base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df['comment'] contains tokenized sentences\n",
    "sentences = df['words'].tolist()\n",
    "\n",
    "# Train Word2Vec model using summed sentence vectors\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Save the model in word2vec format\n",
    "model.wv.save_word2vec_format('word2vec_model.bin', binary=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavra1 = 'like'\n",
    "palavra2 = 'hate'\n",
    "\n",
    "similaridade = modelo_cbow.similarity(palavra1, palavra2)\n",
    "print(f'A similaridade entre \"{palavra1}\" e \"{palavra2}\" é: {similaridade}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Função para somar os vetores de palavras da base\n",
    "\n",
    "Como nossa base é pautada em frases e não em palavras soltas, para que o modelo word2ved faça mais sentido, nós devemos fazer uma função para somar as palavras presentes em determinada frase e assim conseguir um vetor resultade da frase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o modelo Word2Vec previamente treinado\n",
    "model = KeyedVectors.load_word2vec_format('word2vec_model.bin', binary=True, unicode_errors='ignore')\n",
    "\n",
    "# Supondo que df['comment'] contém sentenças tokenizadas\n",
    "sentences = df['words'].tolist()\n",
    "\n",
    "# Função para somar os vetores de palavras de uma frase\n",
    "def sum_word_vectors(sentence):\n",
    "    # Inicializar um vetor de zeros com a mesma dimensão dos vetores de palavras no modelo\n",
    "    sentence_vector = np.zeros(model.vector_size)\n",
    "    \n",
    "    # Contagem do número de palavras na frase que estão no vocabulário do modelo\n",
    "    words_count = 0\n",
    "    \n",
    "    # Para cada palavra na frase, se ela estiver no vocabulário do modelo, somar seu vetor ao vetor da frase\n",
    "    for word in sentence:\n",
    "        if word in model:\n",
    "            sentence_vector += model[word]\n",
    "            words_count += 1\n",
    "    \n",
    "    # Normalizar o vetor da frase dividindo pela contagem de palavras\n",
    "    if words_count != 0:\n",
    "        sentence_vector /= words_count\n",
    "    \n",
    "    return sentence_vector\n",
    "\n",
    "# Calcular os vetores das frases\n",
    "sentences_vectors = [sum_word_vectors(sentence) for sentence in sentences]\n",
    "\n",
    "# Printar os vetores das frases\n",
    "for i, (sentence, vector) in enumerate(zip(sentences, sentences_vectors)):\n",
    "    print(f\"Frase {i+1}:\")\n",
    "    print(f\"  Tokens: {' '.join(sentence)}\")\n",
    "    print(f\"  Vetor: {vector}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultado e Análise dos Testes\n",
    "\n",
    "**Desempenho dos Modelos**\n",
    "\n",
    "- Word2Vec com Dados Pré-processados: Os modelos treinados com dados pré-processados (tokenizados e limpos) apresentaram melhor desempenho em comparação com os dados não processados.\n",
    "\n",
    "- Vetores Pré-treinados vs. Dados da Base: Modelos que utilizaram vetores pré-treinados como ponto de partida superaram aqueles treinados apenas com nossa base de dados, devido à limitação da quantidade de dados disponíveis.\n",
    "\n",
    "- CBOW vs. Skip-gram: Ambos os modelos apresentaram bons resultados. No entanto, o Skip-gram mostrou uma similaridade maior entre \"like\" e \"hate\", o que é justificado pelo mesmo ter sido treinado com nossa base de dados, que é limitada, enquanto o CBOW utiliza os vetores pré-treinados da Google.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Análise de Similaridade: like e hate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A seguir, foi feita uma análise e alguns testes para entender a alta similaridade entre as palavras \"hate\" e \"like\", o que parece inconsistente, mais detalhes acerca de possíveis razões para a alta similaridade estão descritos na seção 3.6.7 da documentação. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Passos para uma melhor análise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Explorar os contextos das palavras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "like_contexts = df[df['comment'].str.contains('like', na=False)]['comment']\n",
    "hate_contexts = df[df['comment'].str.contains('hate', na=False)]['comment']\n",
    "\n",
    "print(\"Contextos de 'like':\")\n",
    "for context in like_contexts:\n",
    "    print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nContextos de 'hate':\")\n",
    "for context in hate_contexts:\n",
    "    print(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Visualização dos Vetores\n",
    "\n",
    "Utilizar técnicas de visualização como PCA (Análise de Componentes Principais) para projetar os vetores em um espaço de menor dimensão e visualizar a relação entre diferentes palavras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Palavras para análise\n",
    "words = ['like', 'hate', 'love', 'do not like']\n",
    "\n",
    "# Filtrando palavras que estão no vocabulário do modelo\n",
    "available_words = [word for word in words if word in model_sg]\n",
    "word_vectors = [model_sg[word] for word in available_words]\n",
    "\n",
    "# Verificar se todas as palavras foram encontradas no vocabulário\n",
    "missing_words = set(words) - set(available_words)\n",
    "if missing_words:\n",
    "    print(f\"As seguintes palavras não foram encontradas no vocabulário do modelo: {missing_words}\")\n",
    "\n",
    "# Realizar PCA para reduzir a dimensionalidade dos vetores\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(word_vectors)\n",
    "\n",
    "# Plotar os vetores\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "\n",
    "for i, word in enumerate(available_words):\n",
    "    plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpretação do Gráfico\n",
    "\n",
    "1. Proximidade no Espaço Vetorial:\n",
    "\n",
    "No gráfico, as palavras \"like\" e \"hate\" estão relativamente distantes uma da outra, enquanto \"love\" está mais separada de ambas.\n",
    "A distância entre \"like\" e \"hate\" no espaço vetorial indica que, apesar da alta similaridade calculada inicialmente, no espaço de vetores de menor dimensão, elas não são tão próximas. Isso sugere que a similaridade pode ser influenciada por outros fatores além da mera proximidade no espaço vetorial.\n",
    "\n",
    "2. Análise de Contexto:\n",
    "\n",
    "A proximidade das palavras no gráfico pode refletir a maneira como elas são usadas no corpus. \"Love\" e \"hate\" são palavras com significados opostos e, idealmente, deveriam estar mais distantes uma da outra. No entanto, a similaridade calculada pode indicar que elas compartilham contextos emocionais intensos, mesmo sendo diferentes.\n",
    "\n",
    "A visualização indica que \"like\" e \"love\" estão mais próximos, o que é esperado, pois ambas expressam sentimentos positivos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Avaliar a Influência de Parâmetros do Modelo\n",
    "\n",
    "Variar parâmetros como vector_size, window, min_count, e epochs para ver como eles afetam a similaridade entre \"like\" e \"hate\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_sg_tuned = Word2Vec(sentences, vector_size=200, window=10, min_count=2, sg=1, workers=4, epochs=5)\n",
    "similaridade_tuned = model_sg_tuned.wv.similarity('like', 'hate')\n",
    "print(f'A similaridade entre \"like\" e \"hate\" após ajustar parâmetros é: {similaridade_tuned}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Comparar com Modelos Diferentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fasttext = FastText(sentences, vector_size=100, window=5, min_count=1, sg=1, workers=4, epochs=10)\n",
    "similaridade_fasttext = model_fasttext.wv.similarity('like', 'hate')\n",
    "print(f'A similaridade entre \"like\" e \"hate\" usando FastText é: {similaridade_fasttext}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusão**\n",
    "\n",
    "A conclusão geral sobre os resultados está detalhada na seção 3.6.7 da documentação. Os pontos abordados são: similaridade calculada, visualização dos vetores, influência do contexto semântico, importância do pré-processamento e parâmetros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dividir os Dados em Conjuntos de Treino e Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nessa fase, separamos os dados coletados em dois grupos: um para treinar nosso modelo e outro para testá-lo depois. Isso ajuda a garantir que nosso modelo aprenda com um conjunto de dados e seja capaz de fazer boas previsões sobre dados novos e desconhecidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo para numpy array para facilitar a manipulação\n",
    "X = np.array(sentences_vectors)\n",
    "y = df['sentiment']\n",
    "\n",
    "# Dividir os dados em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Treinar os Modelos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta etapa, usamos dois tipos diferentes de modelos para entender melhor os sentimentos expressos nas avaliações do Uber:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Naive Bayes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo é bom para lidar com palavras e textos. Ele assume que cada palavra contribui de forma independente para o sentimento da avaliação, o que nos ajuda a calcular a probabilidade de um comentário ser positivo ou negativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar e treinar o modelo Naive Bayes\n",
    "bayes_model = GaussianNB()\n",
    "bayes_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Svm**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este modelo tenta encontrar a melhor linha (ou fronteira) que separa os comentários positivos dos negativos. É como desenhar uma linha reta no meio de pontos em um gráfico para distinguir entre duas categorias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100], \n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], \n",
    "    'degree': [2, 3, 4], \n",
    "    'gamma': ['scale', 'auto'] \n",
    "}\n",
    "\n",
    "svm_model = SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Use GridSearchCV para encontrar os melhores hiperparâmetros\n",
    "grid_search = GridSearchCV(estimator=svm_model, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_search.best_params_)\n",
    "best_svm_model = grid_search.best_estimator_\n",
    "best_svm_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Avaliar o Modelo Naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após treinar nossos modelos, o próximo passo é testar como eles performam com dados que eles nunca viram antes. Para isso, seguimos estes passos:\n",
    "\n",
    "1. **Prever os Sentimentos no Conjunto de Teste**: Usamos o modelo Naive Bayes, que foi treinado anteriormente, para prever os sentimentos nas avaliações do conjunto de teste. Essas previsões são armazenadas em `y_pred`.\n",
    "\n",
    "2. **Calcular a Precisão**: A precisão é uma medida que nos diz qual porcentagem das previsões estava correta. Calculamos isso usando a função `accuracy_score`, comparando as previsões `y_pred` com as verdadeiras respostas `y_test`.\n",
    "\n",
    "3. **Gerar e Imprimir o Relatório de Classificação**: O relatório de classificação fornece mais detalhes sobre a performance do modelo, como precisão, recall e a pontuação F1 para cada classe (positivo, negativo). Usamos a função `classification_report` para obter e imprimir este relatório.\n",
    "\n",
    "Essas métricas nos ajudam a entender quão bem o modelo está trabalhando e em quais áreas ele pode ser melhorado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prever os sentimentos no conjunto de teste\n",
    "y_pred = bayes_model.predict(X_test)\n",
    "\n",
    "# Calcular a precisão\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Gerar e imprimir o relatório de classificação\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Avaliar Modelo SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de treinar o modelo SVM, precisamos verificar como ele se comporta com dados que não foram usados no treinamento. Veja os passos a seguir:\n",
    "\n",
    "1. **Prever os Sentimentos no Conjunto de Teste**: Utilizamos o modelo SVM para fazer previsões sobre as avaliações no conjunto de teste. Guardamos essas previsões na variável `y_pred_svm`.\n",
    "\n",
    "2. **Calcular a Precisão**: Para entender quão precisas são essas previsões, calculamos a precisão usando a função `accuracy_score`. Essa função compara as previsões `y_pred_svm` com as respostas reais `y_test` para calcular a porcentagem de acertos.\n",
    "\n",
    "3. **Gerar e Imprimir o Relatório de Classificação**: Para uma análise mais detalhada, geramos um relatório de classificação com a função `classification_report`. Esse relatório mostra a precisão, o recall e a pontuação F1 para cada classe de sentimento (positivo, negativo). Isso nos ajuda a entender melhor as forças e fraquezas do modelo em classificar corretamente os sentimentos.\n",
    "\n",
    "Esse processo ajuda a avaliar a eficácia do modelo SVM em identificar corretamente os sentimentos nas avaliações, fornecendo insights valiosos para possíveis ajustes no modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prever os sentimentos no conjunto de teste\n",
    "y_pred = best_svm_model.predict(X_test)\n",
    "\n",
    "# Calcular a precisão\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Gerar e imprimir o relatório de classificação\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "O modelo SVM Linear, usando a técnica de bigrams no processo de BoW, foi o mais eficaz para a análise de sentimentos das avaliações do Uber. Essa abordagem alcançou uma precisão geral de 76.43% com resultados sólidos em todas as categorias de sentimentos. Continuaremos refinando o modelo para melhorar sua precisão e capacidade de generalização."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
