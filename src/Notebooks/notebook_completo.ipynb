{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook completo\n",
    "\n",
    "Neste arquivo, encontra-se o modelo escolhido para compor a entrega final do projeto. O algoritmo apresentado foi selecionado com base em seus resultados, que se destacaram entre todos os modelos testados, considerando nossas métricas de avaliação (accuracy, precision, recall e F1-score).\n",
    "\n",
    "Vale ressaltar que o modelo presente neste arquivo é o que está integrado em nossa API e realiza a classificação de sentimentos. O arquivo está dividido nas seguintes etapas:\n",
    "\n",
    "- Importação de bibliotecas\n",
    "- Pré-processamento dos dados\n",
    "- Vetorização dos dados\n",
    "- Modelo de Machine Learning\n",
    "- Exportação do modelo\n",
    "\n",
    "O conteúdo abrange todos os elementos essenciais para realização de testes e treino do modelo, também realiza todas as etapas necessárias para o modelo, como pré-processamento, entre outras etapas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A importação de bibliotecas é o primeiro passo em um projeto de machine learning. Consiste em trazer para o ambiente de trabalho os pacotes e módulos necessários que contêm funções e ferramentas pré-construídas. Essas bibliotecas facilitam a manipulação de dados, a construção de modelos, a análise estatística e a visualização de resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import string\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from spellchecker import SpellChecker\n",
    "from nltk import pos_tag\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Baixar os recursos necessários do NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-Processamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O pré-processamento dos dados envolve a preparação dos dados brutos para que possam ser usados de maneira eficaz pelo modelo de machine learning. O objetivo é melhorar a qualidade dos dados, utilizando de técnicas como, remoção de stopwords, tokenização, entre outras, o que, por sua vez, melhora o desempenho do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Importação dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obter o diretório atual (funciona no Jupyter Notebook)\n",
    "current_dir = os.getcwd()\n",
    "csv_path = os.path.join(current_dir, 'dados.csv')\n",
    "# Carregar o arquivo CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenização é o processo de dividir um texto em unidades menores chamadas tokens. Esses tokens podem ser palavras individuais, partes de palavras ou até mesmo caracteres, dependendo do nível de granularidade desejado. A tokenização é uma etapa fundamental no processamento de linguagem natural (PLN), sendo essencial para a preparação, análise e manipulação de texto em uma variedade de aplicações, incluindo análise de sentimento, classificação de texto e tradução automática. Ao dividir o texto em tokens, os dados tornam-se estruturados e adequados para análise, facilitando a extração de informações e a modelagem de soluções de PLN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokeniza o DataFrame em uma lista de tokens.\n",
    "\n",
    "    Args:\n",
    "    text (DataFrame): Comentários a serem tokenizados.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de tokens.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "def filter_empty_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Remove listas vazias ou com espaços vazios.\n",
    "\n",
    "    Args:\n",
    "    tokens (list): Lista de tokens.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de tokens não vazios.\n",
    "    \"\"\"\n",
    "    return [token for token in tokens if token.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lemmatização é o processo de reduzir palavras a sua forma base ou lema, considerando o contexto e a morfologia da língua. Essa técnica é importante em PLN para tratar diferentes formas de uma palavra como iguais, como \"corre\" e \"correu\" ambas reduzidas a \"correr\". Isso simplifica o processamento de texto e melhora a análise em tarefas como recuperação de informações e análise de sentimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Retorna o tag correspondente do WordNet para o tag do Treebank do Penn.\n",
    "\n",
    "    Args:\n",
    "        treebank_tag (str): O tag do Treebank do Penn que precisa ser convertido.\n",
    "\n",
    "    Returns:\n",
    "        str: O tag correspondente do WordNet (ADJ, VERB, NOUN, ADV).\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN \n",
    "\n",
    "def lemmatize_tokens_with_pos(tokens):\n",
    "    \"\"\"\n",
    "    Lemmatiza uma lista de tokens baseando-se em sua parte do discurso.\n",
    "\n",
    "    Args:\n",
    "    tokens (list of str): Tokens a serem lematizados.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de lemas das palavras.\n",
    "    \"\"\"\n",
    "    # Criar um Doc do spaCy a partir dos tokens\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "\n",
    "    # Lemmatiza usando o spaCy\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remoção de pontuação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A remoção de pontuação é um processo utilizado no pré-processamento de texto que visa eliminar caracteres de pontuação, como vírgulas, pontos e pontos de exclamação, de um texto. Isso é feito para limpar o texto e reduzir a dimensionalidade dos dados, facilitando a análise e a modelagem. Ao remover a pontuação, os tokens resultantes contêm apenas palavras ou partes de palavras, tornando-os mais adequados para tarefas de processamento de linguagem natural."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_from_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Remove pontuações de uma lista de tokens e exclui os tokens que consistem exclusivamente de caracteres de pontuação.\n",
    "\n",
    "    Args:\n",
    "    tokens (list): Lista de tokens a serem processados.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de tokens sem pontuações.\n",
    "    \"\"\"\n",
    "    # Regex para identificar pontuações\n",
    "    regex_punctuation = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "    # Remove pontuações de cada token e filtra tokens que ficaram vazios ou são apenas pontuações\n",
    "    tokens_no_punct = [regex_punctuation.sub('', token) for token in tokens]\n",
    "    tokens_no_punct = [token for token in tokens_no_punct if token.strip() != '']\n",
    "\n",
    "    return tokens_no_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remoção de números"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A remoção de números é um passo comum no pré-processamento de texto que consiste em eliminar todos os caracteres numéricos de um texto. Isso é feito para limpar o texto de informações numéricas que podem não ser relevantes para a análise ou para garantir que as palavras sejam tratadas de maneira uniforme durante a tokenização. Ao remover números, os tokens resultantes contêm apenas palavras e outros caracteres não numéricos, simplificando o texto para análise e modelagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers_from_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Remove todos os dígitos de uma lista de tokens e remove os tokens que consistem exclusivamente de números.\n",
    "\n",
    "    Args:\n",
    "    tokens (list): Lista de tokens a serem processados.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de tokens sem números.\n",
    "    \"\"\"\n",
    "    # Remover dígitos de cada token e depois filtrar os tokens que são apenas números ou ficaram vazios\n",
    "    tokens_no_numbers = [re.sub(r'\\d+', '', token) for token in tokens]\n",
    "    tokens_no_numbers = [token for token in tokens_no_numbers if token.strip() != '']\n",
    "    return tokens_no_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remoção de Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords são palavras que são frequentemente removidas durante o pré-processamento de texto em tarefas de Processamento de Linguagem Natural (PLN). Essas palavras são geralmente as mais comuns em um idioma, como ‘é’, ‘em’, ‘um’, ‘e’ em português, ou ‘is’, ‘in’, ‘a’, ‘and’ em inglês, e tendem a aparecer em quase todos os documentos de um corpus.\n",
    "\n",
    "A remoção de stopwords é uma prática comum porque essas palavras, embora muito frequentes, geralmente não carregam muito significado e podem adicionar ruído aos dados. Além disso, removendo-as, podemos reduzir o tamanho do nosso vocabulário e, consequentemente, o espaço de recursos, tornando nossos modelos de PLN mais eficientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_preserve_adverbs_conjunctions(tokens):\n",
    "    \"\"\"\n",
    "    Remove stopwords de uma lista de tokens enquanto preserva todos os advérbios e conjunções, \n",
    "    que são cruciais para manter o fluxo lógico e a estrutura das frases.\n",
    "\n",
    "    Args:\n",
    "        tokens (list): Lista de tokens a serem processados.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de tokens sem stopwords, com todos os advérbios e conjunções preservados.\n",
    "        \n",
    "    \"\"\"\n",
    "    if not isinstance(tokens, list):\n",
    "        raise TypeError(\"Input tokens must be a list\")\n",
    "    \n",
    "    # Loading stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('uber')\n",
    "    \n",
    "    # Classifying tokens by parts of speech\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    \n",
    "    # Filtering tokens to remove stopwords but keep adverbs (tagged as 'RB') and conjunctions (tagged as 'CC' for coordinating, 'IN' for subordinating)\n",
    "    filtered_tokens = [\n",
    "        word for word, tag in tagged_tokens \n",
    "        if word.lower() not in stop_words \n",
    "        or tag.startswith('RB')  # Adverbs\n",
    "        or tag in ('CC', 'IN')  # Conjunctions\n",
    "    ]\n",
    "\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remoção de URL's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A remoção de links é uma etapa de pré-processamento em tarefas de Processamento de Linguagem Natural (PLN) que envolve a eliminação de URLs ou links de um texto. Isso é feito para reduzir o ruído nos dados e focar nas palavras e frases que carregam mais significado.\n",
    "\n",
    "Links geralmente não contribuem para a semântica de um texto e podem ser bastante variados e únicos, o que pode adicionar ruído e aumentar a dimensionalidade dos dados. Além disso, os links podem levar a conteúdo externo que está fora do contexto do texto atual, tornando a análise mais complexa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a função para remover URLs\n",
    "def remove_urls(text):\n",
    "    \"\"\"\n",
    "    Remove URLs de um texto fornecido.\n",
    "\n",
    "    Args:\n",
    "    texto (list): O texto de entrada contendo URLs.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de tokens com URLs removidas.\n",
    "    \"\"\"\n",
    "    return re.sub(r'http\\S+|www.\\S+', '', text, flags=re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correção de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A etapa de correção de texto no processamento de linguagem natural (NLP) envolve a identificação e correção de erros gramaticais em um texto. Isso inclui a correção de erros de concordância verbal e nominal, uso incorreto de tempos verbais, pontuação inadequada e outros aspectos gramaticais. Ferramentas de correção gramatical utilizam algoritmos e modelos linguísticos para analisar a estrutura das sentenças e sugerir melhorias, garantindo que o texto final seja gramaticalmente correto e claro. Esta etapa é crucial para melhorar a qualidade e a legibilidade do texto, especialmente em aplicações como assistentes de escrita, corretores ortográficos e sistemas de tradução automática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_checker(words):\n",
    "    \"\"\"\n",
    "    Corrige a ortografia de uma lista de palavras usando pyspellchecker,\n",
    "    com exceções para palavras específicas. Garante que nenhum valor None seja retornado.\n",
    "\n",
    "    Args:\n",
    "        words (list of str): Lista de palavras a serem corrigidas.\n",
    "\n",
    "    Returns:\n",
    "        str: Texto com a ortografia corrigida.\n",
    "    \"\"\"\n",
    "    spell = SpellChecker()\n",
    "    exceptions = {'uber'}\n",
    "    corrected_words = []\n",
    "\n",
    "    for word in words:\n",
    "        # Asegurar que a palavra não é None\n",
    "        if word is None:\n",
    "            corrected_words.append('')\n",
    "        elif word.lower() in exceptions:\n",
    "            corrected_words.append(word)\n",
    "        else:\n",
    "            # Obter a melhor sugestão de correção\n",
    "            corrected_word = spell.correction(word) if word else ''\n",
    "            corrected_words.append(corrected_word if corrected_word else '')\n",
    "\n",
    "    # Assegurar que todos os elementos sejam strings\n",
    "    corrected_words = [w if w is not None else '' for w in corrected_words]\n",
    "    corrected_text = ' '.join(corrected_words)\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Contrações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A etapa de correção de contrações envolve a expansão de palavras contraídas em suas formas completas. Por exemplo, transformar \"can't\" em \"cannot\" ou \"it's\" em \"it is\". Essa etapa é importante no processamento de linguagem natural (NLP) porque as contrações podem introduzir ambiguidades e dificuldades na análise de texto. Ao expandir as contrações, o texto se torna mais claro e consistente para os algoritmos, melhorando a precisão em tarefas como análise de sentimentos, tradução automática e extração de informações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    \"\"\"\n",
    "    Expande contrações em um texto dado com base em um mapeamento fornecido.\n",
    "\n",
    "    Args:\n",
    "        text (str): Texto que contém contrações.\n",
    "        contraction_mapping (dict): Um dicionário mapeando contrações para suas formas expandidas.\n",
    "\n",
    "    Returns:\n",
    "        str: Texto com todas as contrações expandidas de acordo com o mapeamento.\n",
    "    \"\"\"\n",
    "    contraction_map = {\n",
    "    \"can't\": \"can not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"they'll\": \"they will\",\n",
    "}\n",
    "    \n",
    "    for contraction, expansion in contraction_map.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "    return text\n",
    "\n",
    "# Exemplo de uso\n",
    "sample_text = \"I can't do this anymore, because it's too hard.\"\n",
    "expanded_text = expand_contractions(sample_text)\n",
    "print(expanded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Balanceamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A etapa de balanceamento das classes de dados é uma técnica usada em aprendizado de máquina e processamento de linguagem natural (NLP) para lidar com conjuntos de dados desbalanceados, onde uma ou mais classes têm significativamente mais exemplos do que outras. Esse desbalanceamento pode prejudicar o desempenho dos modelos, tornando-os tendenciosos em favor das classes majoritárias. Para balancear as classes, algumas estratégias comuns incluem subamostragem, que reduz o número de exemplos na classe majoritária (como remover 20% dos dados da classe negativa), e superamostragem, que aumenta o número de exemplos na classe minoritária (como duplicar os dados da classe positiva). Outra técnica é a geração de exemplos sintéticos, criando novos exemplos para a classe minoritária usando métodos como SMOTE (Synthetic Minority Over-sampling Technique). Essas técnicas ajudam a garantir que o modelo de aprendizado de máquina receba uma representação mais equilibrada de todas as classes, melhorando sua capacidade de generalizar e performar de maneira mais justa e precisa em dados não vistos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_balance_negatives_and_multiply_positives(df):\n",
    "    \"\"\"\n",
    "    Remove outliers do comprimento dos comentários para a classe de sentimentos negativos (-1),\n",
    "    baseando-se nos quartis da própria classe negativa, e remove aleatoriamente 40% dos dados negativos\n",
    "    restantes para ajudar no balanceamento das classes. Multiplica os dados da classe positiva (1) por 2,5 vezes\n",
    "    para aumentar sua representatividade no dataset.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame contendo as colunas 'sentiment', 'comment', e 'comment_length'.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame com outliers removidos da classe negativa, redução estratégica de 40% dos negativos,\n",
    "                    e multiplicação dos dados positivos por 2,5 vezes.\n",
    "    \"\"\"\n",
    "    # Adiciona a coluna 'comment_length' ao DataFrame se não existir\n",
    "    if 'comment_length' not in df.columns:\n",
    "        df['comment_length'] = df['comment'].apply(len)\n",
    "\n",
    "    # Adiciona a coluna 'word_count'\n",
    "    df['word_count'] = df['comment'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    # Filtra os dados para a classe negativa (-1)\n",
    "    negativos = df[df['sentiment'] == -1]\n",
    "    \n",
    "    # Calcula os quartis apenas para a classe negativa\n",
    "    Q1 = negativos['comment_length'].quantile(0.25)\n",
    "    Q3 = negativos['comment_length'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Filtra os outliers na classe negativa baseado no limite calculado\n",
    "    negativos_filtrados = negativos[negativos['comment_length'] <= upper_bound]\n",
    "    \n",
    "    # Amostragem aleatória para remover 20% dos dados negativos filtrados\n",
    "    negativos_reduzidos = negativos_filtrados.sample(frac=0.83, random_state=42)\n",
    "\n",
    "    # Multiplica os dados da classe positiva (1) por 2,5 vezes\n",
    "    positivos = df[df['sentiment'] == 1]\n",
    "    positivos_multiplicados = pd.concat([positivos] * 2 + [positivos.sample(frac=0.5, random_state=42)], ignore_index=True)\n",
    "    \n",
    "    # Combina os dados reduzidos de sentimentos negativos com as outras classes\n",
    "    df_final = pd.concat([negativos_reduzidos, df[df['sentiment'] == 0], positivos_multiplicados], ignore_index=True)\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Criação de nova coluna "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A coluna words_sentiment é adicionada ao DataFrame para indicar o sentimento de cada comentário com base na presença de palavras positivas ou negativas. O valor da coluna é determinado da seguinte maneira: 1 se o comentário contiver qualquer palavra positiva, -1 se o comentário contiver qualquer palavra negativa, e 0 se o comentário não contiver nem palavras positivas nem negativas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_sentiment_words(df):\n",
    "    \"\"\"\n",
    "    Verifica a presença de palavras positivas e negativas em cada comentário de um DataFrame.\n",
    "\n",
    "    Args:\n",
    "    - df: DataFrame contendo os comentários a serem analisados.\n",
    "\n",
    "    Returns:\n",
    "    Um novo DataFrame com uma coluna 'words_sentiment', onde cada entrada indica o sentimento do comentário:\n",
    "    - 1 se houver palavras positivas no comentário.\n",
    "    - -1 se houver palavras negativas no comentário.\n",
    "    - 0 se não houver nem palavras positivas nem negativas no comentário.\n",
    "    \"\"\"\n",
    "    # Lista de palavras negativas e positivas\n",
    "    negative_words = [\n",
    "        'bad', 'terrible', 'awful', 'hate', 'not', 'hard', 'lose',\n",
    "    'disaster', 'worst', 'poor', 'negative'\n",
    "    ]\n",
    "    positive_words = [\n",
    "        'good', 'great', 'excellent', 'love', 'awesome', 'amazing',\n",
    "    'best', 'positive', 'nice', 'happy'\n",
    "    ]  \n",
    "   \n",
    "    \n",
    "    # Função para verificar a presença de palavras negativas e positivas em um comentário\n",
    "    def check_sentiment_words(text):\n",
    "        \"\"\"\n",
    "        Verifica se um texto contém palavras com conotação negativa ou positiva.\n",
    "\n",
    "        Args:\n",
    "            text (str): O texto a ser analisado.\n",
    "\n",
    "        Returns:\n",
    "            int: Um valor inteiro indicando o sentimento do texto. Retorna -1 se o texto contém apenas palavras negativas,\n",
    "                1 se contém apenas palavras positivas, e 0 se não contém nem palavras negativas nem positivas.\n",
    "        \"\"\"\n",
    "\n",
    "        # Verifica se há palavras negativas\n",
    "        has_negative = any(word in text for word in negative_words)\n",
    "        # Verifica se há palavras positivas\n",
    "        has_positive = any(word in text for word in positive_words)\n",
    "        \n",
    "        # Retorna -1 se negativas, 1 se positivas, e 0 se não houver nem negativas nem positivas\n",
    "        if has_negative and not has_positive:\n",
    "            return -1\n",
    "        elif has_positive and not has_negative:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    # Aplicar a função a cada comentário e atribuir o resultado a uma nova coluna\n",
    "    df['words_sentiment'] = df['comment'].apply(check_sentiment_words)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o pipeline para pré-processamento dos comentários\n",
    "pipeline = Pipeline([\n",
    "    ('expand_contractions', FunctionTransformer(lambda x: x.apply(expand_contractions))),\n",
    "    ('url_remover', FunctionTransformer(lambda x: x.apply(remove_urls))),\n",
    "    ('tokenizer', FunctionTransformer(lambda x: x.apply(tokenize_text))),\n",
    "    ('lemmatizacao', FunctionTransformer(lambda x: x.apply(lemmatize_tokens_with_pos))),\n",
    "    ('punctuation_remover', FunctionTransformer(lambda x: x.apply(remove_punctuation_from_tokens))),\n",
    "    ('number_remover', FunctionTransformer(lambda x: x.apply(remove_numbers_from_tokens))),\n",
    "    ('stopwords_remover', FunctionTransformer(lambda x: x.apply(remove_stopwords_preserve_adverbs_conjunctions))),\n",
    "    ('filter_empty_tokens', FunctionTransformer(lambda x: x.apply(filter_empty_tokens))),\n",
    "    ('spell_checker', FunctionTransformer(lambda x: x.apply(spell_checker))),\n",
    "])\n",
    "\n",
    "# Primeiro aplicar a remoção de outliers e a redução de dados negativos\n",
    "df = remove_outliers_balance_negatives_and_multiply_positives(df)\n",
    "# Aplicar o pipeline ao DataFrame existente\n",
    "df['comment'] = pipeline.fit_transform(df['comment'])\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exportar dados pré-processados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file_path = 'dados_pre_processados.csv'\n",
    "df.to_csv(output_file_path, index=False)\n",
    "\n",
    "print(f\"Arquivo CSV exportado para {output_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelo Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta etapa, é construído e treinado o modelo de machine learning usando os dados vetorizados. O modelo pode ser de diferentes tipos, como regressão logística, máquinas de vetor de suporte (SVM), redes neurais, entre outros. O processo inclui a escolha do algoritmo, o ajuste dos hiperparâmetros, o treinamento com dados de treino e a validação com dados de teste para avaliar o desempenho do modelo com base em métricas como accuracy, precision, recall e F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "csv_path = os.path.join(current_dir, 'dados_pre_processados.csv')\n",
    "df = pd.read_csv(csv_path)\n",
    "df.head()\n",
    "df.drop(\"words_sentiment\", axis=1, inplace=True)\n",
    "df['comment'] = df['comment'].apply(str)\n",
    "\n",
    "# Função para vetorização usando bigramas\n",
    "def sklearn_bow_bigrams(docs):\n",
    "    \"\"\"\n",
    "    Cria uma matriz de termos do documento usando a representação Bag of Words com bigramas (sequências de dois termos consecutivos).\n",
    "\n",
    "    Args:\n",
    "        docs (list): Uma lista de documentos (strings) para os quais a matriz Bag of Words deve ser criada.\n",
    "\n",
    "    Returns:\n",
    "        Uma tupla contendo a matriz Bag of Words e o objeto CountVectorizer treinado.\n",
    "\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 4))\n",
    "    bow_matrix = vectorizer.fit_transform(docs)\n",
    "    return bow_matrix, vectorizer\n",
    "\n",
    "# Gerar matriz BoW usando a coluna 'comment'\n",
    "X, vectorizer = sklearn_bow_bigrams(df['comment'])\n",
    "\n",
    "# Dividir o dataset em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['sentiment'], test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definindo os estimadores com os melhores parâmetros encontrados\n",
    "best_estimators = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=150, max_depth=20, random_state=42)),\n",
    "    ('et', ExtraTreesClassifier(n_estimators=150, max_depth=None, random_state=42)), \n",
    "    ('logreg', LogisticRegression())  # O parâmetro C é usado apenas no meta-modelo\n",
    "]\n",
    "\n",
    "# Definindo o meta-modelo com o melhor parâmetro C encontrado\n",
    "final_estimator = LogisticRegression(C=10000.0)\n",
    "\n",
    "# Definindo o modelo de stacking com os melhores parâmetros\n",
    "best_stack_model = StackingClassifier(\n",
    "    estimators=best_estimators,\n",
    "    final_estimator=final_estimator,\n",
    "    stack_method='auto'\n",
    ")\n",
    "\n",
    "# Treinamento do modelo de stacking com os melhores parâmetros\n",
    "best_stack_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar a função cohen_kappa_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "# Prever os sentimentos no conjunto de teste\n",
    "y_pred = best_stack_model.predict(X_test)\n",
    "\n",
    "# Calcular a precisão\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Gerar e imprimir o relatório de classificação\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Calcular o Kappa de Cohen\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "print(\"Cohen's Kappa:\", kappa)\n",
    "\n",
    "# Rótulos reais usados no dataset\n",
    "labels = [-1, 0, 1]\n",
    "\n",
    "# Criar a matriz de confusão com rótulos específicos\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "\n",
    "# Plotar a matriz de confusão usando Seaborn\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues', cbar=False, xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exportação do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a construção e validação, o modelo final é exportado para ser utilizado em um ambiente de produção. Isso envolve salvar o modelo treinado em um formato que pode ser facilmente carregado e executado em aplicações externas, no caso em nossa API. A exportação garante que o modelo possa ser reutilizado sem a necessidade de ser re-treinado, facilitando a sua integração em sistemas de classificação de sentimentos em tempo real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_test(text):\n",
    "    \"\"\"\n",
    "    Aplica as etapas de pré-processamento de texto a um texto de entrada.\n",
    "\n",
    "    Args:\n",
    "        text (str): O texto de entrada a ser pré-processado.\n",
    "\n",
    "    Returns:\n",
    "        list: Uma lista de tokens após o pré-processamento.\n",
    "    \"\"\"\n",
    "    text = remove_urls(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    tokens = lemmatize_tokens_with_pos(tokens)\n",
    "    tokens = remove_punctuation_from_tokens(tokens)\n",
    "    tokens = remove_numbers_from_tokens(tokens)\n",
    "    tokens = remove_stopwords_preserve_adverbs_conjunctions(tokens)\n",
    "    tokens = filter_empty_tokens(tokens)\n",
    "    tokens = spell_checker(tokens)\n",
    "    return tokens\n",
    "\n",
    "# Exemplo de uso\n",
    "sample_text = \"Unsafe\"\n",
    "pipeline_test(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar o modelo e o vetorizador em um arquivo pickle\n",
    "with open('stack_model.pkl', 'wb') as file:\n",
    "    pickle.dump(best_stack_model, file)\n",
    "\n",
    "with open('vectorizer.pkl', 'wb') as file:\n",
    "    pickle.dump(vectorizer, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_path = 'stack_model.pkl'\n",
    "vectorizer_path = 'vectorizer.pkl'\n",
    "\n",
    "with open(modelo_path, 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "with open(vectorizer_path, 'rb') as file:\n",
    "    loaded_vectorizer = pickle.load(file)\n",
    "\n",
    "def preprocessar_texto(texto):\n",
    "    \"\"\"\n",
    "    Pré-processa o texto usando uma pipeline de processamento definida.\n",
    "\n",
    "    Args:\n",
    "    texto (str): Texto a ser processado.\n",
    "\n",
    "    Returns:\n",
    "    str: Texto processado.\n",
    "    \"\"\"\n",
    "    # Supondo que 'pipeline_test' seja uma função definida em outro lugar que processa o texto\n",
    "    return pipeline_test(texto)\n",
    "\n",
    "def fazer_previsao(comentario):\n",
    "    \"\"\"\n",
    "    Faz previsão de um comentário usando o modelo e o vetorizador carregados.\n",
    "\n",
    "    Args:\n",
    "    comentario (str): Comentário para o qual se deseja fazer a previsão.\n",
    "\n",
    "    Returns:\n",
    "    str: Previsão do modelo para o comentário.\n",
    "    \"\"\"\n",
    "    texto_processado = preprocessar_texto(comentario)\n",
    "    comentario_vetorizado = loaded_vectorizer.transform([texto_processado])\n",
    "    previsao = loaded_model.predict(comentario_vetorizado)\n",
    "    return previsao[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
