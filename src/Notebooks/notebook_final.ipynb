{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Final\n",
    "\n",
    "Neste arquivo, encontra-se o modelo escolhido para compor a entrega final do projeto. O algoritmo apresentado foi selecionado com base em seus resultados, que se destacaram entre todos os modelos testados, considerando nossas métricas de avaliação (accuracy, precision, recall e F1-score).\n",
    "\n",
    "Vale ressaltar que o modelo presente neste arquivo é o que está integrado em nossa API e realiza a classificação de sentimentos. O arquivo está dividido nas seguintes etapas:\n",
    "\n",
    "- Importação de bibliotecas\n",
    "- Pré-processamento dos dados\n",
    "- Vetorização dos dados\n",
    "- Modelo de Machine Learning\n",
    "- Exportação do modelo\n",
    "\n",
    "O conteúdo abrange apenas os elementos essenciais para nossa API, apenas utilizando dos arquivos gerados pelo Notebook completo para realizar as previsões."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importação das bibliotecas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A importação de bibliotecas é o primeiro passo em um projeto de machine learning. Consiste em trazer para o ambiente de trabalho os pacotes e módulos necessários que contêm funções e ferramentas pré-construídas. Essas bibliotecas facilitam a manipulação de dados, a construção de modelos, a análise estatística e a visualização de resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "import string\n",
    "import pickle\n",
    "from nltk import pos_tag\n",
    "from spellchecker import SpellChecker\n",
    "import numpy as np\n",
    "\n",
    "# Baixar os recursos necessários do NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pré-Processamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O pré-processamento dos dados envolve a preparação dos dados brutos para que possam ser usados de maneira eficaz pelo modelo de machine learning. O objetivo é melhorar a qualidade dos dados, utilizando de técnicas como, remoção de stopwords, tokenização, entre outras, o que, por sua vez, melhora o desempenho do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tokenização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A tokenização é o processo de dividir um texto em unidades menores chamadas tokens. Esses tokens podem ser palavras individuais, partes de palavras ou até mesmo caracteres, dependendo do nível de granularidade desejado. A tokenização é uma etapa fundamental no processamento de linguagem natural (PLN), sendo essencial para a preparação, análise e manipulação de texto em uma variedade de aplicações, incluindo análise de sentimento, classificação de texto e tradução automática. Ao dividir o texto em tokens, os dados tornam-se estruturados e adequados para análise, facilitando a extração de informações e a modelagem de soluções de PLN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    \"\"\"\n",
    "    Tokeniza o DataFrame em uma lista de tokens.\n",
    "\n",
    "    Args:\n",
    "    text (DataFrame): Comentários a serem tokenizados.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de tokens.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    return tokens\n",
    "\n",
    "def filter_empty_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Remove listas vazias ou com espaços vazios.\n",
    "\n",
    "    Args:\n",
    "    tokens (list): Lista de tokens.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de tokens não vazios.\n",
    "    \"\"\"\n",
    "    return [token for token in tokens if token.strip()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lemmatização é o processo de reduzir palavras a sua forma base ou lema, considerando o contexto e a morfologia da língua. Essa técnica é importante em PLN para tratar diferentes formas de uma palavra como iguais, como \"corre\" e \"correu\" ambas reduzidas a \"correr\". Isso simplifica o processamento de texto e melhora a análise em tarefas como recuperação de informações e análise de sentimento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Retorna o tag correspondente do WordNet para o tag do Treebank do Penn.\n",
    "\n",
    "    Args:\n",
    "        treebank_tag (str): O tag do Treebank do Penn que precisa ser convertido.\n",
    "\n",
    "    Returns:\n",
    "        str: O tag correspondente do WordNet (ADJ, VERB, NOUN, ADV).\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN \n",
    "\n",
    "def lemmatize_tokens_with_pos(tokens):\n",
    "    \"\"\"\n",
    "    Lemmatiza uma lista de tokens baseando-se em sua parte do discurso.\n",
    "\n",
    "    Args:\n",
    "    tokens (list of str): Tokens a serem lematizados.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de lemas das palavras.\n",
    "    \"\"\"\n",
    "    # Criar um Doc do spaCy a partir dos tokens\n",
    "    doc = nlp(\" \".join(tokens))\n",
    "\n",
    "    # Lemmatiza usando o spaCy\n",
    "    lemmas = [token.lemma_ for token in doc]\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remoção de pontuação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A remoção de pontuação é um processo utilizado no pré-processamento de texto que visa eliminar caracteres de pontuação, como vírgulas, pontos e pontos de exclamação, de um texto. Isso é feito para limpar o texto e reduzir a dimensionalidade dos dados, facilitando a análise e a modelagem. Ao remover a pontuação, os tokens resultantes contêm apenas palavras ou partes de palavras, tornando-os mais adequados para tarefas de processamento de linguagem natural.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation_from_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Remove pontuações de uma lista de tokens e exclui os tokens que consistem exclusivamente de caracteres de pontuação.\n",
    "\n",
    "    Args:\n",
    "    tokens (list): Lista de tokens a serem processados.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de tokens sem pontuações.\n",
    "    \"\"\"\n",
    "    # Regex para identificar pontuações\n",
    "    regex_punctuation = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "\n",
    "    # Remove pontuações de cada token e filtra tokens que ficaram vazios ou são apenas pontuações\n",
    "    tokens_no_punct = [regex_punctuation.sub('', token) for token in tokens]\n",
    "    tokens_no_punct = [token for token in tokens_no_punct if token.strip() != '']\n",
    "\n",
    "    return tokens_no_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remoção de números"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A remoção de números é um passo comum no pré-processamento de texto que consiste em eliminar todos os caracteres numéricos de um texto. Isso é feito para limpar o texto de informações numéricas que podem não ser relevantes para a análise ou para garantir que as palavras sejam tratadas de maneira uniforme durante a tokenização. Ao remover números, os tokens resultantes contêm apenas palavras e outros caracteres não numéricos, simplificando o texto para análise e modelagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers_from_tokens(tokens):\n",
    "    \"\"\"\n",
    "    Remove todos os dígitos de uma lista de tokens e remove os tokens que consistem exclusivamente de números.\n",
    "\n",
    "    Args:\n",
    "    tokens (list): Lista de tokens a serem processados.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de tokens sem números.\n",
    "    \"\"\"\n",
    "    # Remover dígitos de cada token e depois filtrar os tokens que são apenas números ou ficaram vazios\n",
    "    tokens_no_numbers = [re.sub(r'\\d+', '', token) for token in tokens]\n",
    "    tokens_no_numbers = [token for token in tokens_no_numbers if token.strip() != '']\n",
    "    return tokens_no_numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remoção de Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords são palavras que são frequentemente removidas durante o pré-processamento de texto em tarefas de Processamento de Linguagem Natural (PLN). Essas palavras são geralmente as mais comuns em um idioma, como ‘é’, ‘em’, ‘um’, ‘e’ em português, ou ‘is’, ‘in’, ‘a’, ‘and’ em inglês, e tendem a aparecer em quase todos os documentos de um corpus.\n",
    "\n",
    "A remoção de stopwords é uma prática comum porque essas palavras, embora muito frequentes, geralmente não carregam muito significado e podem adicionar ruído aos dados. Além disso, removendo-as, podemos reduzir o tamanho do nosso vocabulário e, consequentemente, o espaço de recursos, tornando nossos modelos de PLN mais eficientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_preserve_adverbs_conjunctions(tokens):\n",
    "    \"\"\"\n",
    "    Remove stopwords de uma lista de tokens enquanto preserva todos os advérbios e conjunções, \n",
    "    que são cruciais para manter o fluxo lógico e a estrutura das frases.\n",
    "\n",
    "    Args:\n",
    "        tokens (list): Lista de tokens a serem processados.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de tokens sem stopwords, com todos os advérbios e conjunções preservados.\n",
    "        \n",
    "    \"\"\"\n",
    "    if not isinstance(tokens, list):\n",
    "        raise TypeError(\"Input tokens must be a list\")\n",
    "    \n",
    "    # Loading stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    # Classifying tokens by parts of speech\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    \n",
    "    # Filtering tokens to remove stopwords but keep adverbs (tagged as 'RB') and conjunctions (tagged as 'CC' for coordinating, 'IN' for subordinating)\n",
    "    filtered_tokens = [\n",
    "        word for word, tag in tagged_tokens \n",
    "        if word.lower() not in stop_words \n",
    "        or tag.startswith('RB')  # Adverbs\n",
    "        or tag in ('CC', 'IN')  # Conjunctions\n",
    "    ]\n",
    "\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remoção de URL's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A remoção de links é uma etapa de pré-processamento em tarefas de Processamento de Linguagem Natural (PLN) que envolve a eliminação de URLs ou links de um texto. Isso é feito para reduzir o ruído nos dados e focar nas palavras e frases que carregam mais significado.\n",
    "\n",
    "Links geralmente não contribuem para a semântica de um texto e podem ser bastante variados e únicos, o que pode adicionar ruído e aumentar a dimensionalidade dos dados. Além disso, os links podem levar a conteúdo externo que está fora do contexto do texto atual, tornando a análise mais complexa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir a função para remover URLs\n",
    "def remove_urls(text):\n",
    "    \"\"\"\n",
    "    Remove URLs de um texto fornecido.\n",
    "\n",
    "    Args:\n",
    "    texto (list): O texto de entrada contendo URLs.\n",
    "\n",
    "    Returns:\n",
    "    list: Lista de tokens com URLs removidas.\n",
    "    \"\"\"\n",
    "    return re.sub(r'http\\S+|www.\\S+', '', text, flags=re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Correção de texto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A etapa de correção de texto no processamento de linguagem natural (NLP) envolve a identificação e correção de erros gramaticais em um texto. Isso inclui a correção de erros de concordância verbal e nominal, uso incorreto de tempos verbais, pontuação inadequada e outros aspectos gramaticais. Ferramentas de correção gramatical utilizam algoritmos e modelos linguísticos para analisar a estrutura das sentenças e sugerir melhorias, garantindo que o texto final seja gramaticalmente correto e claro. Esta etapa é crucial para melhorar a qualidade e a legibilidade do texto, especialmente em aplicações como assistentes de escrita, corretores ortográficos e sistemas de tradução automática."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_checker(words):\n",
    "    \"\"\"\n",
    "    Corrige a ortografia de uma lista de palavras usando pyspellchecker,\n",
    "    com exceções para palavras específicas. Garante que nenhum valor None seja retornado.\n",
    "\n",
    "    Args:\n",
    "        words (list of str): Lista de palavras a serem corrigidas.\n",
    "\n",
    "    Returns:\n",
    "        str: Texto com a ortografia corrigida.\n",
    "    \"\"\"\n",
    "    spell = SpellChecker()\n",
    "    exceptions = {'uber', 'unsafe'}\n",
    "    corrected_words = []\n",
    "\n",
    "    for word in words:\n",
    "        # Asegurar que a palavra não é None\n",
    "        if word is None:\n",
    "            corrected_words.append('')\n",
    "        elif word.lower() in exceptions:\n",
    "            corrected_words.append(word)\n",
    "        else:\n",
    "            # Obter a melhor sugestão de correção\n",
    "            corrected_word = spell.correction(word) if word else ''\n",
    "            corrected_words.append(corrected_word if corrected_word else '')\n",
    "\n",
    "    # Assegurar que todos os elementos sejam strings\n",
    "    corrected_words = [w if w is not None else '' for w in corrected_words]\n",
    "    corrected_text = ' '.join(corrected_words)\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Contrações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A etapa de correção de contrações envolve a expansão de palavras contraídas em suas formas completas. Por exemplo, transformar \"can't\" em \"cannot\" ou \"it's\" em \"it is\". Essa etapa é importante no processamento de linguagem natural (NLP) porque as contrações podem introduzir ambiguidades e dificuldades na análise de texto. Ao expandir as contrações, o texto se torna mais claro e consistente para os algoritmos, melhorando a precisão em tarefas como análise de sentimentos, tradução automática e extração de informações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(text):\n",
    "    \"\"\"\n",
    "    Expande contrações em um texto dado com base em um mapeamento fornecido.\n",
    "\n",
    "    Args:\n",
    "        text (str): Texto que contém contrações.\n",
    "        contraction_mapping (dict): Um dicionário mapeando contrações para suas formas expandidas.\n",
    "\n",
    "    Returns:\n",
    "        str: Texto com todas as contrações expandidas de acordo com o mapeamento.\n",
    "    \"\"\"\n",
    "    contraction_map = {\n",
    "    \"can't\": \"can not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"can't've\": \"cannot have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"mightn't've\": \"might not have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"i'd\": \"i would\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"he'd\": \"he would\",\n",
    "    \"she'd\": \"she would\",\n",
    "    \"we'd\": \"we would\",\n",
    "    \"they'd\": \"they would\",\n",
    "    \"i'll\": \"i will\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"he'll\": \"he will\",\n",
    "    \"she'll\": \"she will\",\n",
    "    \"we'll\": \"we will\",\n",
    "    \"they'll\": \"they will\",\n",
    "}\n",
    "    \n",
    "    for contraction, expansion in contraction_map.items():\n",
    "        text = text.replace(contraction, expansion)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Balanceamento dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A etapa de balanceamento das classes de dados é uma técnica usada em aprendizado de máquina e processamento de linguagem natural (NLP) para lidar com conjuntos de dados desbalanceados, onde uma ou mais classes têm significativamente mais exemplos do que outras. Esse desbalanceamento pode prejudicar o desempenho dos modelos, tornando-os tendenciosos em favor das classes majoritárias. Para balancear as classes, algumas estratégias comuns incluem subamostragem, que reduz o número de exemplos na classe majoritária (como remover 40% dos dados da classe negativa), e superamostragem, que aumenta o número de exemplos na classe minoritária (como duplicar os dados da classe positiva). Outra técnica é a geração de exemplos sintéticos, criando novos exemplos para a classe minoritária usando métodos como SMOTE (Synthetic Minority Over-sampling Technique). Essas técnicas ajudam a garantir que o modelo de aprendizado de máquina receba uma representação mais equilibrada de todas as classes, melhorando sua capacidade de generalizar e performar de maneira mais justa e precisa em dados não vistos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers_balance_negatives_and_multiply_positives(df):\n",
    "    \"\"\"\n",
    "    Remove outliers do comprimento dos comentários para a classe de sentimentos negativos (-1),\n",
    "    baseando-se nos quartis da própria classe negativa, e remove aleatoriamente 40% dos dados negativos\n",
    "    restantes para ajudar no balanceamento das classes. Multiplica os dados da classe positiva (1) por 2,5 vezes\n",
    "    para aumentar sua representatividade no dataset.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): DataFrame contendo as colunas 'sentiment', 'comment', e 'comment_length'.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: DataFrame com outliers removidos da classe negativa, redução estratégica de 40% dos negativos,\n",
    "                    e multiplicação dos dados positivos por 2,5 vezes.\n",
    "    \"\"\"\n",
    "    # Adiciona a coluna 'comment_length' ao DataFrame se não existir\n",
    "    if 'comment_length' not in df.columns:\n",
    "        df['comment_length'] = df['comment'].apply(len)\n",
    "\n",
    "    # Adiciona a coluna 'word_count'\n",
    "    df['word_count'] = df['comment'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    # Filtra os dados para a classe negativa (-1)\n",
    "    negativos = df[df['sentiment'] == -1]\n",
    "    \n",
    "    # Calcula os quartis apenas para a classe negativa\n",
    "    Q1 = negativos['comment_length'].quantile(0.25)\n",
    "    Q3 = negativos['comment_length'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    # Filtra os outliers na classe negativa baseado no limite calculado\n",
    "    negativos_filtrados = negativos[negativos['comment_length'] <= upper_bound]\n",
    "    \n",
    "    # Amostragem aleatória para remover 25% dos dados negativos filtrados\n",
    "    negativos_reduzidos = negativos_filtrados.sample(frac=0.80, random_state=42)\n",
    "\n",
    "    # Multiplica os dados da classe positiva (1) por 2,5 vezes\n",
    "    positivos = df[df['sentiment'] == 1]\n",
    "    positivos_multiplicados = pd.concat([positivos] * 2 + [positivos.sample(frac=0.5, random_state=42)], ignore_index=True)\n",
    "    \n",
    "    # Combina os dados reduzidos de sentimentos negativos com as outras classes\n",
    "    df_final = pd.concat([negativos_reduzidos, df[df['sentiment'] == 0], positivos_multiplicados], ignore_index=True)\n",
    "\n",
    "    return df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exportação do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após a construção e validação, o modelo final é exportado para ser utilizado em um ambiente de produção. Isso envolve salvar o modelo treinado em um formato que pode ser facilmente carregado e executado em aplicações externas, no caso em nossa API. A exportação garante que o modelo possa ser reutilizado sem a necessidade de ser re-treinado, facilitando a sua integração em sistemas de classificação de sentimentos em tempo real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_test(text):\n",
    "    \"\"\"\n",
    "    Aplica as etapas de pré-processamento de texto a um texto de entrada.\n",
    "\n",
    "    Args:\n",
    "        text (str): O texto de entrada a ser pré-processado.\n",
    "\n",
    "    Returns:\n",
    "        list: Uma lista de tokens após o pré-processamento.\n",
    "    \"\"\"\n",
    "    text = remove_urls(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    tokens = lemmatize_tokens_with_pos(tokens)\n",
    "    tokens = remove_punctuation_from_tokens(tokens)\n",
    "    tokens = remove_numbers_from_tokens(tokens)\n",
    "    tokens = remove_stopwords_preserve_adverbs_conjunctions(tokens)\n",
    "    tokens = filter_empty_tokens(tokens)\n",
    "    tokens = spell_checker(tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_path = 'stack_model.pkl'\n",
    "vectorizer_path = 'vectorizer.pkl'\n",
    "\n",
    "with open(modelo_path, 'rb') as file:\n",
    "    loaded_model = pickle.load(file)\n",
    "\n",
    "with open(vectorizer_path, 'rb') as file:\n",
    "    loaded_vectorizer = pickle.load(file)\n",
    "\n",
    "def preprocessar_texto(texto):\n",
    "    \"\"\"\n",
    "    Preprocessa o texto usando uma pipeline de processamento definida.\n",
    "\n",
    "    Args:\n",
    "    texto (str): Texto a ser processado.\n",
    "\n",
    "    Returns:\n",
    "    str: Texto processado.\n",
    "    \"\"\"\n",
    "    # Supondo que 'pipeline_test' seja uma função definida em outro lugar que processa o texto\n",
    "    return pipeline_test(texto)\n",
    "\n",
    "def vetorizar_texto(texto):\n",
    "    \"\"\"\n",
    "    Transforma o texto em vetor bigrams.\n",
    "\n",
    "    Args:\n",
    "    texto (str): Texto a ser vetorizado.\n",
    "\n",
    "    Returns:\n",
    "    list: Vetor como lista de listas.\n",
    "    \"\"\"\n",
    "    comentario_vetorizado = loaded_vectorizer.transform([texto])\n",
    "    return comentario_vetorizado.toarray().tolist()\n",
    "\n",
    "def classificacao_sentimento_modelo(comentario_vetorizado):\n",
    "    \"\"\"\n",
    "    Faz uma previsão usando o vetor.\n",
    "\n",
    "    Args:\n",
    "    comentario_vetorizado (np.array): Vetor de comentário, deve ser um array 1D.\n",
    "\n",
    "    Returns:\n",
    "    int: Previsão de sentimento.\n",
    "    \"\"\"\n",
    "    # Verifica se o array é 1D e o reformata para 2D\n",
    "    if comentario_vetorizado.ndim == 1:\n",
    "        comentario_vetorizado = comentario_vetorizado.reshape(1, -1)\n",
    "\n",
    "    previsao = loaded_model.predict(comentario_vetorizado)\n",
    "    return int(previsao[0])  # Converte para int, garantindo compatibilidade com JSON\n",
    "\n",
    "def fazer_previsao(comentario):\n",
    "    \"\"\"\n",
    "    Faz previsão de um comentário usando o modelo e o vetorizador carregados.\n",
    "\n",
    "    Args:\n",
    "    comentario (str): Comentário para o qual se deseja fazer a previsão.\n",
    "\n",
    "    Returns:\n",
    "    str: Previsão do modelo para o comentário.\n",
    "    \"\"\"\n",
    "    texto_processado = preprocessar_texto(comentario)\n",
    "    comentario_vetorizado = loaded_vectorizer.transform([texto_processado])\n",
    "    previsao = loaded_model.predict(comentario_vetorizado)\n",
    "    return previsao[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
